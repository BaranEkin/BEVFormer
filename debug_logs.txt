
 ENTERED BEVFORMER HEAD INIT : 

 ENTERED BEVFORMER HEAD INIT_LAYERS : 

 Printing inside the BEVFormer_Head _init_layers______________________________________________ : 

 CLS branch: : [Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=10, bias=True)]

 REG branch: : Sequential(
  (0): Linear(in_features=256, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): ReLU()
  (4): Linear(in_features=256, out_features=10, bias=True)
)

 Printing inside the BEVFormer_Head init___________________________________________________ : 

 __dict__ of BEVFormer Head : {'bev_h': 200, 'bev_w': 200, 'fp16_enabled': False, 'with_box_refine': True, 'as_two_stage': False, 'code_size': 10, 'bbox_coder': <projects.mmdet3d_plugin.core.bbox.coders.nms_free_coder.NMSFreeCoder object at 0x7fdf260e98e0>, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'real_w': 102.4, 'real_h': 102.4, 'num_cls_fcs': 1, 'training': True, '_parameters': OrderedDict([('code_weights', Parameter containing:
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000,
        0.2000]))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict([('loss_cls', FocalLoss()), ('loss_bbox', L1Loss()), ('loss_iou', GIoULoss()), ('activate', ReLU(inplace=True)), ('positional_encoding', LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)), ('transformer', PerceptionTransformer(
  (encoder): BEVFormerEncoder(
    (layers): ModuleList(
      (0): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): DetectionTransformerDecoder(
    (layers): ModuleList(
      (0): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (can_bus_mlp): Sequential(
    (0): Linear(in_features=18, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=256, bias=True)
    (3): ReLU(inplace=True)
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)), ('cls_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('reg_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('bev_embedding', Embedding(40000, 256)), ('query_embedding', Embedding(900, 512))]), '_is_init': False, 'init_cfg': None, 'bg_cls_weight': 0, 'sync_cls_avg_factor': True, 'num_query': 900, 'num_classes': 10, 'in_channels': 256, 'num_reg_fcs': 2, 'train_cfg': None, 'test_cfg': None, 'cls_out_channels': 10, 'act_cfg': {'type': 'ReLU', 'inplace': True}, 'embed_dims': 256}

 ENTERED BEVFORMER HEAD INIT : 

 ENTERED BEVFORMER HEAD INIT_LAYERS : 

 Printing inside the BEVFormer_Head _init_layers______________________________________________ : 

 CLS branch: : [Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=10, bias=True)]

 REG branch: : Sequential(
  (0): Linear(in_features=256, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): ReLU()
  (4): Linear(in_features=256, out_features=10, bias=True)
)

 Printing inside the BEVFormer_Head init___________________________________________________ : 

 __dict__ of BEVFormer Head : {'bev_h': 200, 'bev_w': 200, 'fp16_enabled': False, 'with_box_refine': True, 'as_two_stage': False, 'code_size': 10, 'bbox_coder': <projects.mmdet3d_plugin.core.bbox.coders.nms_free_coder.NMSFreeCoder object at 0x7fbd84167970>, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'real_w': 102.4, 'real_h': 102.4, 'num_cls_fcs': 1, 'training': True, '_parameters': OrderedDict([('code_weights', Parameter containing:
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000,
        0.2000]))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict([('loss_cls', FocalLoss()), ('loss_bbox', L1Loss()), ('loss_iou', GIoULoss()), ('activate', ReLU(inplace=True)), ('positional_encoding', LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)), ('transformer', PerceptionTransformer(
  (encoder): BEVFormerEncoder(
    (layers): ModuleList(
      (0): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): DetectionTransformerDecoder(
    (layers): ModuleList(
      (0): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (can_bus_mlp): Sequential(
    (0): Linear(in_features=18, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=256, bias=True)
    (3): ReLU(inplace=True)
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)), ('cls_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('reg_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('bev_embedding', Embedding(40000, 256)), ('query_embedding', Embedding(900, 512))]), '_is_init': False, 'init_cfg': None, 'bg_cls_weight': 0, 'sync_cls_avg_factor': True, 'num_query': 900, 'num_classes': 10, 'in_channels': 256, 'num_reg_fcs': 2, 'train_cfg': None, 'test_cfg': None, 'cls_out_channels': 10, 'act_cfg': {'type': 'ReLU', 'inplace': True}, 'embed_dims': 256}

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD INIT : 

 ENTERED BEVFORMER HEAD INIT_LAYERS : 

 Printing inside the BEVFormer_Head _init_layers______________________________________________ : 

 CLS branch: : [Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=10, bias=True)]

 REG branch: : Sequential(
  (0): Linear(in_features=256, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): ReLU()
  (4): Linear(in_features=256, out_features=10, bias=True)
)

 Printing inside the BEVFormer_Head init___________________________________________________ : 

 __dict__ of BEVFormer Head : {'bev_h': 200, 'bev_w': 200, 'fp16_enabled': False, 'with_box_refine': True, 'as_two_stage': False, 'code_size': 10, 'bbox_coder': <projects.mmdet3d_plugin.core.bbox.coders.nms_free_coder.NMSFreeCoder object at 0x7fad52b55bb0>, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'real_w': 102.4, 'real_h': 102.4, 'num_cls_fcs': 1, 'training': True, '_parameters': OrderedDict([('code_weights', Parameter containing:
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000,
        0.2000]))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict([('loss_cls', FocalLoss()), ('loss_bbox', L1Loss()), ('loss_iou', GIoULoss()), ('activate', ReLU(inplace=True)), ('positional_encoding', LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)), ('transformer', PerceptionTransformer(
  (encoder): BEVFormerEncoder(
    (layers): ModuleList(
      (0): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
              (attention_weights): Linear(in_features=256, out_features=256, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): DetectionTransformerDecoder(
    (layers): ModuleList(
      (0): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (can_bus_mlp): Sequential(
    (0): Linear(in_features=18, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=256, bias=True)
    (3): ReLU(inplace=True)
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)), ('cls_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('reg_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('bev_embedding', Embedding(40000, 256)), ('query_embedding', Embedding(900, 512))]), '_is_init': False, 'init_cfg': None, 'bg_cls_weight': 0, 'sync_cls_avg_factor': True, 'num_query': 900, 'num_classes': 10, 'in_channels': 256, 'num_reg_fcs': 2, 'train_cfg': None, 'test_cfg': None, 'cls_out_channels': 10, 'act_cfg': {'type': 'ReLU', 'inplace': True}, 'embed_dims': 256}

 ENTERED BEVFORMER HEAD INIT : 

 ENTERED BEVFORMER HEAD INIT_LAYERS : 

 Printing inside the BEVFormer_Head _init_layers______________________________________________ : 

 CLS branch: : [Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=10, bias=True)]

 REG branch: : Sequential(
  (0): Linear(in_features=256, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): ReLU()
  (4): Linear(in_features=256, out_features=10, bias=True)
)

 Printing inside the BEVFormer_Head init___________________________________________________ : 

 __dict__ of BEVFormer Head : {'bev_h': 50, 'bev_w': 50, 'fp16_enabled': False, 'with_box_refine': True, 'as_two_stage': False, 'code_size': 10, 'bbox_coder': <projects.mmdet3d_plugin.core.bbox.coders.nms_free_coder.NMSFreeCoder object at 0x7fd7c2137820>, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'real_w': 102.4, 'real_h': 102.4, 'num_cls_fcs': 1, 'training': True, '_parameters': OrderedDict([('code_weights', Parameter containing:
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000,
        0.2000]))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict([('loss_cls', FocalLoss()), ('loss_bbox', L1Loss()), ('loss_iou', GIoULoss()), ('activate', ReLU(inplace=True)), ('positional_encoding', LearnedPositionalEncoding(num_feats=128, row_num_embed=50, col_num_embed=50)), ('transformer', PerceptionTransformer(
  (encoder): BEVFormerEncoder(
    (layers): ModuleList(
      (0): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): DetectionTransformerDecoder(
    (layers): ModuleList(
      (0): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (can_bus_mlp): Sequential(
    (0): Linear(in_features=18, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=256, bias=True)
    (3): ReLU(inplace=True)
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)), ('cls_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('reg_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('bev_embedding', Embedding(2500, 256)), ('query_embedding', Embedding(900, 512))]), '_is_init': False, 'init_cfg': None, 'bg_cls_weight': 0, 'sync_cls_avg_factor': True, 'num_query': 900, 'num_classes': 10, 'in_channels': 256, 'num_reg_fcs': 2, 'train_cfg': None, 'test_cfg': None, 'cls_out_channels': 10, 'act_cfg': {'type': 'ReLU', 'inplace': True}, 'embed_dims': 256}

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD INIT : 

 ENTERED BEVFORMER HEAD INIT_LAYERS : 

 Printing inside the BEVFormer_Head _init_layers______________________________________________ : 

 CLS branch: : [Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=10, bias=True)]

 REG branch: : Sequential(
  (0): Linear(in_features=256, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): ReLU()
  (4): Linear(in_features=256, out_features=10, bias=True)
)

 Printing inside the BEVFormer_Head init___________________________________________________ : 

 __dict__ of BEVFormer Head : {'bev_h': 50, 'bev_w': 50, 'fp16_enabled': False, 'with_box_refine': True, 'as_two_stage': False, 'code_size': 10, 'bbox_coder': <projects.mmdet3d_plugin.core.bbox.coders.nms_free_coder.NMSFreeCoder object at 0x7f70dab996a0>, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'real_w': 102.4, 'real_h': 102.4, 'num_cls_fcs': 1, 'training': True, '_parameters': OrderedDict([('code_weights', Parameter containing:
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000,
        0.2000]))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict([('loss_cls', FocalLoss()), ('loss_bbox', L1Loss()), ('loss_iou', GIoULoss()), ('activate', ReLU(inplace=True)), ('positional_encoding', LearnedPositionalEncoding(num_feats=128, row_num_embed=50, col_num_embed=50)), ('transformer', PerceptionTransformer(
  (encoder): BEVFormerEncoder(
    (layers): ModuleList(
      (0): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): DetectionTransformerDecoder(
    (layers): ModuleList(
      (0): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (can_bus_mlp): Sequential(
    (0): Linear(in_features=18, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=256, bias=True)
    (3): ReLU(inplace=True)
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)), ('cls_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('reg_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('bev_embedding', Embedding(2500, 256)), ('query_embedding', Embedding(900, 512))]), '_is_init': False, 'init_cfg': None, 'bg_cls_weight': 0, 'sync_cls_avg_factor': True, 'num_query': 900, 'num_classes': 10, 'in_channels': 256, 'num_reg_fcs': 2, 'train_cfg': None, 'test_cfg': None, 'cls_out_channels': 10, 'act_cfg': {'type': 'ReLU', 'inplace': True}, 'embed_dims': 256}

 ENTERED BEVFORMER HEAD INIT : 

 ENTERED BEVFORMER HEAD INIT_LAYERS : 

 Printing inside the BEVFormer_Head _init_layers______________________________________________ : 

 CLS branch: : [Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=10, bias=True)]

 REG branch: : Sequential(
  (0): Linear(in_features=256, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): ReLU()
  (4): Linear(in_features=256, out_features=10, bias=True)
)

 Printing inside the BEVFormer_Head init___________________________________________________ : 

 __dict__ of BEVFormer Head : {'bev_h': 50, 'bev_w': 50, 'fp16_enabled': False, 'with_box_refine': True, 'as_two_stage': False, 'code_size': 10, 'bbox_coder': <projects.mmdet3d_plugin.core.bbox.coders.nms_free_coder.NMSFreeCoder object at 0x7f9b8381b220>, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'real_w': 102.4, 'real_h': 102.4, 'num_cls_fcs': 1, 'training': True, '_parameters': OrderedDict([('code_weights', Parameter containing:
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000,
        0.2000]))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict([('loss_cls', FocalLoss()), ('loss_bbox', L1Loss()), ('loss_iou', GIoULoss()), ('activate', ReLU(inplace=True)), ('positional_encoding', LearnedPositionalEncoding(num_feats=128, row_num_embed=50, col_num_embed=50)), ('transformer', PerceptionTransformer(
  (encoder): BEVFormerEncoder(
    (layers): ModuleList(
      (0): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): DetectionTransformerDecoder(
    (layers): ModuleList(
      (0): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (can_bus_mlp): Sequential(
    (0): Linear(in_features=18, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=256, bias=True)
    (3): ReLU(inplace=True)
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)), ('cls_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('reg_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('bev_embedding', Embedding(2500, 256)), ('query_embedding', Embedding(900, 512))]), '_is_init': False, 'init_cfg': None, 'bg_cls_weight': 0, 'sync_cls_avg_factor': True, 'num_query': 900, 'num_classes': 10, 'in_channels': 256, 'num_reg_fcs': 2, 'train_cfg': None, 'test_cfg': None, 'cls_out_channels': 10, 'act_cfg': {'type': 'ReLU', 'inplace': True}, 'embed_dims': 256}

 ENTERED BEVFORMER HEAD INIT : 

 ENTERED BEVFORMER HEAD INIT_LAYERS : 

 Printing inside the BEVFormer_Head _init_layers______________________________________________ : 

 CLS branch: : [Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=10, bias=True)]

 REG branch: : Sequential(
  (0): Linear(in_features=256, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): ReLU()
  (4): Linear(in_features=256, out_features=10, bias=True)
)

 Printing inside the BEVFormer_Head init___________________________________________________ : 

 __dict__ of BEVFormer Head : {'bev_h': 50, 'bev_w': 50, 'fp16_enabled': False, 'with_box_refine': True, 'as_two_stage': False, 'code_size': 10, 'bbox_coder': <projects.mmdet3d_plugin.core.bbox.coders.nms_free_coder.NMSFreeCoder object at 0x7fb8805c9100>, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'real_w': 102.4, 'real_h': 102.4, 'num_cls_fcs': 1, 'training': True, '_parameters': OrderedDict([('code_weights', Parameter containing:
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000,
        0.2000]))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict([('loss_cls', FocalLoss()), ('loss_bbox', L1Loss()), ('loss_iou', GIoULoss()), ('activate', ReLU(inplace=True)), ('positional_encoding', LearnedPositionalEncoding(num_feats=128, row_num_embed=50, col_num_embed=50)), ('transformer', PerceptionTransformer(
  (encoder): BEVFormerEncoder(
    (layers): ModuleList(
      (0): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): DetectionTransformerDecoder(
    (layers): ModuleList(
      (0): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (can_bus_mlp): Sequential(
    (0): Linear(in_features=18, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=256, bias=True)
    (3): ReLU(inplace=True)
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)), ('cls_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('reg_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('bev_embedding', Embedding(2500, 256)), ('query_embedding', Embedding(900, 512))]), '_is_init': False, 'init_cfg': None, 'bg_cls_weight': 0, 'sync_cls_avg_factor': True, 'num_query': 900, 'num_classes': 10, 'in_channels': 256, 'num_reg_fcs': 2, 'train_cfg': None, 'test_cfg': None, 'cls_out_channels': 10, 'act_cfg': {'type': 'ReLU', 'inplace': True}, 'embed_dims': 256}

 ENTERED BEVFORMER HEAD INIT : 

 ENTERED BEVFORMER HEAD INIT_LAYERS : 

 Printing inside the BEVFormer_Head _init_layers______________________________________________ : 

 CLS branch: : [Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=10, bias=True)]

 REG branch: : Sequential(
  (0): Linear(in_features=256, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): ReLU()
  (4): Linear(in_features=256, out_features=10, bias=True)
)

 Printing inside the BEVFormer_Head init___________________________________________________ : 

 __dict__ of BEVFormer Head : {'bev_h': 50, 'bev_w': 50, 'fp16_enabled': False, 'with_box_refine': True, 'as_two_stage': False, 'code_size': 10, 'bbox_coder': <projects.mmdet3d_plugin.core.bbox.coders.nms_free_coder.NMSFreeCoder object at 0x7f808ca181f0>, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'real_w': 102.4, 'real_h': 102.4, 'num_cls_fcs': 1, 'training': True, '_parameters': OrderedDict([('code_weights', Parameter containing:
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000,
        0.2000]))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict([('loss_cls', FocalLoss()), ('loss_bbox', L1Loss()), ('loss_iou', GIoULoss()), ('activate', ReLU(inplace=True)), ('positional_encoding', LearnedPositionalEncoding(num_feats=128, row_num_embed=50, col_num_embed=50)), ('transformer', PerceptionTransformer(
  (encoder): BEVFormerEncoder(
    (layers): ModuleList(
      (0): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): DetectionTransformerDecoder(
    (layers): ModuleList(
      (0): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (can_bus_mlp): Sequential(
    (0): Linear(in_features=18, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=256, bias=True)
    (3): ReLU(inplace=True)
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)), ('cls_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('reg_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('bev_embedding', Embedding(2500, 256)), ('query_embedding', Embedding(900, 512))]), '_is_init': False, 'init_cfg': None, 'bg_cls_weight': 0, 'sync_cls_avg_factor': True, 'assigner': <projects.mmdet3d_plugin.core.bbox.assigners.hungarian_assigner_3d.HungarianAssigner3D object at 0x7f808ca186d0>, 'sampler': <mmdet.core.bbox.samplers.pseudo_sampler.PseudoSampler object at 0x7f808ca18730>, 'num_query': 900, 'num_classes': 10, 'in_channels': 256, 'num_reg_fcs': 2, 'train_cfg': {'grid_size': [512, 512, 1], 'voxel_size': [0.2, 0.2, 8], 'point_cloud_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'out_size_factor': 4, 'assigner': {'type': 'HungarianAssigner3D', 'cls_cost': {'type': 'FocalLossCost', 'weight': 2.0}, 'reg_cost': {'type': 'BBox3DL1Cost', 'weight': 0.25}, 'iou_cost': {'type': 'IoUCost', 'weight': 0.0}, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]}}, 'test_cfg': None, 'cls_out_channels': 10, 'act_cfg': {'type': 'ReLU', 'inplace': True}, 'embed_dims': 256}

 ENTERED BEVFORMER HEAD INIT_WEIGHTS : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 Printing inside the BEVFormer_Head forward___________________________________________________ : 

 Query embedding: : Embedding(900, 512)

 Object query embeds: : Parameter containing:
tensor([[-0.5364, -1.4722, -0.2604,  ...,  0.1764, -0.2588,  0.4041],
        [ 0.4289,  0.5810,  0.6073,  ..., -0.1219,  1.6779, -0.7048],
        [ 1.2946, -0.1301, -1.2199,  ..., -0.0930, -1.0721, -0.7679],
        ...,
        [ 0.9517, -0.1862,  1.3235,  ..., -0.0432,  1.3554, -0.2305],
        [ 0.4156,  1.0228,  0.6179,  ..., -0.8400,  1.4984,  0.2379],
        [ 0.8340,  1.1012,  1.9524,  ..., -0.3306,  0.6511,  0.9604]],
       device='cuda:0', requires_grad=True)

 Object query embeds shape: torch.Size([900, 512]) : 

 BEV Qs: : Parameter containing:
tensor([[-0.8226, -0.9222, -1.8220,  ...,  0.5906, -0.4288, -1.8992],
        [ 0.0433,  0.2351, -0.2583,  ..., -0.7681,  1.0731, -0.1300],
        [ 0.7926,  1.3901,  0.1612,  ...,  1.0087,  1.4410,  0.2607],
        ...,
        [-1.6253,  1.8269, -0.4453,  ...,  0.6100,  1.0617,  0.8081],
        [ 0.3419,  0.4125,  1.0719,  ...,  0.2665, -0.3219,  0.0524],
        [-0.1564,  0.3061, -0.2086,  ..., -0.7349,  0.8385, -0.0694]],
       device='cuda:0', requires_grad=True)

 BEV Qs shape: torch.Size([2500, 256]) : 

 BEV mask: : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')

 BEV mask shape: torch.Size([1, 50, 50]) : 

 BEV pos: : tensor([[[[ 1.4164, -0.7025,  2.6319,  ..., -1.3179, -1.3906,  1.3950],
          [ 1.4164, -0.7025,  2.6319,  ..., -1.3179, -1.3906,  1.3950],
          [ 1.4164, -0.7025,  2.6319,  ..., -1.3179, -1.3906,  1.3950],
          ...,
          [ 1.4164, -0.7025,  2.6319,  ..., -1.3179, -1.3906,  1.3950],
          [ 1.4164, -0.7025,  2.6319,  ..., -1.3179, -1.3906,  1.3950],
          [ 1.4164, -0.7025,  2.6319,  ..., -1.3179, -1.3906,  1.3950]],

         [[ 0.2379,  0.3330, -0.9008,  ..., -0.3727, -0.4176,  0.6152],
          [ 0.2379,  0.3330, -0.9008,  ..., -0.3727, -0.4176,  0.6152],
          [ 0.2379,  0.3330, -0.9008,  ..., -0.3727, -0.4176,  0.6152],
          ...,
          [ 0.2379,  0.3330, -0.9008,  ..., -0.3727, -0.4176,  0.6152],
          [ 0.2379,  0.3330, -0.9008,  ..., -0.3727, -0.4176,  0.6152],
          [ 0.2379,  0.3330, -0.9008,  ..., -0.3727, -0.4176,  0.6152]],

         [[-0.9334, -1.0581,  2.0708,  ..., -0.8441,  0.0228, -2.0538],
          [-0.9334, -1.0581,  2.0708,  ..., -0.8441,  0.0228, -2.0538],
          [-0.9334, -1.0581,  2.0708,  ..., -0.8441,  0.0228, -2.0538],
          ...,
          [-0.9334, -1.0581,  2.0708,  ..., -0.8441,  0.0228, -2.0538],
          [-0.9334, -1.0581,  2.0708,  ..., -0.8441,  0.0228, -2.0538],
          [-0.9334, -1.0581,  2.0708,  ..., -0.8441,  0.0228, -2.0538]],

         ...,

         [[ 1.1648,  1.1648,  1.1648,  ...,  1.1648,  1.1648,  1.1648],
          [ 0.1447,  0.1447,  0.1447,  ...,  0.1447,  0.1447,  0.1447],
          [ 0.5541,  0.5541,  0.5541,  ...,  0.5541,  0.5541,  0.5541],
          ...,
          [ 0.3333,  0.3333,  0.3333,  ...,  0.3333,  0.3333,  0.3333],
          [-0.2068, -0.2068, -0.2068,  ..., -0.2068, -0.2068, -0.2068],
          [-0.7780, -0.7780, -0.7780,  ..., -0.7780, -0.7780, -0.7780]],

         [[ 0.9234,  0.9234,  0.9234,  ...,  0.9234,  0.9234,  0.9234],
          [ 1.9029,  1.9029,  1.9029,  ...,  1.9029,  1.9029,  1.9029],
          [-0.1817, -0.1817, -0.1817,  ..., -0.1817, -0.1817, -0.1817],
          ...,
          [-0.2713, -0.2713, -0.2713,  ..., -0.2713, -0.2713, -0.2713],
          [ 0.0318,  0.0318,  0.0318,  ...,  0.0318,  0.0318,  0.0318],
          [ 0.5115,  0.5115,  0.5115,  ...,  0.5115,  0.5115,  0.5115]],

         [[ 1.3873,  1.3873,  1.3873,  ...,  1.3873,  1.3873,  1.3873],
          [ 0.3904,  0.3904,  0.3904,  ...,  0.3904,  0.3904,  0.3904],
          [-0.2345, -0.2345, -0.2345,  ..., -0.2345, -0.2345, -0.2345],
          ...,
          [ 0.0072,  0.0072,  0.0072,  ...,  0.0072,  0.0072,  0.0072],
          [-0.1746, -0.1746, -0.1746,  ..., -0.1746, -0.1746, -0.1746],
          [-0.5564, -0.5564, -0.5564,  ..., -0.5564, -0.5564, -0.5564]]]],
       device='cuda:0', grad_fn=<RepeatBackward>)

 BEV pos shape: torch.Size([1, 256, 50, 50]) : 

 OUTPUTS : (tensor([[[ 0.1293, -0.8296,  0.7280,  ...,  1.6130,  0.2804, -0.1444]],

        [[ 1.3599,  0.1083,  0.3239,  ...,  1.5450, -0.0049,  0.6069]],

        [[-0.4854,  0.3193,  0.0033,  ...,  0.6170, -0.5052,  1.6112]],

        ...,

        [[-0.0569, -0.8779, -0.2223,  ...,  1.4433,  0.3969,  0.8107]],

        [[-0.4201,  0.0608, -0.6404,  ...,  1.1649,  0.0385,  0.7621]],

        [[-0.7017, -0.4087,  0.1059,  ...,  1.0952,  0.7742,  0.3575]]],
       device='cuda:0', grad_fn=<PermuteBackward>), tensor([[[[ 9.1865e-01,  6.7904e-01,  2.6818e-03,  ..., -2.1358e-01,
           -1.5684e+00,  1.4037e+00]],

         [[ 3.1779e-01,  9.1086e-01,  2.2462e-01,  ...,  4.3400e-01,
            6.3561e-02,  9.5275e-01]],

         [[ 1.0889e-01,  1.4990e+00, -2.3825e-02,  ..., -1.5734e-02,
           -2.4118e+00,  5.3936e-01]],

         ...,

         [[-8.6677e-01,  1.3733e+00, -8.0862e-01,  ..., -1.3793e-01,
           -8.1615e-01,  1.2346e+00]],

         [[ 1.5232e-01, -4.8400e-01, -1.7717e+00,  ..., -7.0706e-01,
            5.2782e-01,  5.6077e-01]],

         [[-1.9340e+00, -1.9470e-01,  8.6850e-03,  ..., -7.5895e-02,
           -1.0061e+00,  1.0252e+00]]],


        [[[ 6.8677e-02,  1.4608e+00,  1.1939e+00,  ...,  1.8261e-01,
            6.8278e-02,  1.0352e+00]],

         [[ 5.4537e-01, -3.0493e-02,  1.0638e+00,  ...,  1.5249e-01,
            1.5753e-01,  5.7713e-01]],

         [[-7.8207e-01,  1.1428e+00,  9.1056e-01,  ...,  6.0364e-01,
           -1.0788e+00,  9.3068e-01]],

         ...,

         [[-1.1196e+00,  6.2639e-01,  3.4110e-01,  ...,  8.8094e-01,
           -2.1589e-01,  5.0976e-01]],

         [[ 2.9788e-01,  1.1660e+00, -5.6606e-01,  ..., -7.8429e-01,
            3.5855e-01,  3.8637e-01]],

         [[-4.0204e-01,  2.5593e-01,  1.1117e+00,  ...,  4.7836e-01,
           -1.2875e+00,  6.8175e-01]]],


        [[[-1.0580e+00,  1.2552e+00,  1.0701e+00,  ..., -6.8423e-01,
            1.1887e+00,  8.5871e-01]],

         [[-1.2986e-01,  2.1618e-01,  1.4692e+00,  ..., -1.3572e+00,
            1.2909e+00, -1.4636e-01]],

         [[-5.7283e-02,  5.6509e-01,  2.4311e+00,  ..., -3.0126e-01,
           -5.7200e-02, -3.4817e-01]],

         ...,

         [[-6.3222e-02,  1.0713e-01,  2.1996e+00,  ..., -3.3853e-01,
            6.7889e-01, -7.6408e-01]],

         [[ 3.6763e-01,  1.4571e-01,  1.4407e+00,  ..., -1.4512e+00,
            7.7339e-01,  1.0878e+00]],

         [[-3.1669e-01, -8.6910e-02,  2.4861e+00,  ...,  7.1207e-01,
           -4.2891e-01,  1.1694e+00]]],


        [[[-8.3941e-01, -3.6318e-01,  9.7229e-01,  ...,  2.3787e+00,
            1.2979e+00,  1.2351e-01]],

         [[-2.6894e-01, -4.6052e-01,  1.6752e+00,  ...,  1.1293e+00,
            1.1138e+00, -5.5676e-01]],

         [[-3.3564e-01, -1.1480e+00,  1.9499e+00,  ...,  7.2977e-01,
            1.1912e+00, -1.0211e+00]],

         ...,

         [[-5.1034e-02, -8.9287e-01,  8.9405e-01,  ...,  4.4623e-01,
            5.8422e-01, -1.8189e-01]],

         [[ 2.2961e-01,  2.1399e-03,  1.8394e+00,  ...,  8.9625e-01,
            8.0100e-01, -4.7298e-01]],

         [[-5.1082e-01, -8.0640e-01,  2.2503e+00,  ...,  2.4807e+00,
            1.4265e+00, -5.4720e-01]]],


        [[[-1.4932e+00, -5.1278e-01,  7.8356e-01,  ...,  1.2221e-01,
            2.8573e-01, -5.1103e-01]],

         [[-1.4191e+00, -2.8925e-01,  1.9581e+00,  ...,  1.5198e+00,
            1.6655e-01, -1.0748e+00]],

         [[-2.3939e-01, -8.5835e-01,  7.5258e-01,  ..., -4.4702e-02,
            3.7167e-01, -5.2002e-01]],

         ...,

         [[-1.0284e+00, -7.2346e-02,  1.1239e+00,  ..., -7.5949e-01,
           -5.8639e-02, -8.5519e-01]],

         [[-2.3468e-01, -1.3461e+00,  1.8793e+00,  ...,  8.5501e-01,
            2.0006e+00,  1.0287e-01]],

         [[-4.1933e-01, -1.4212e+00,  3.9989e-01,  ...,  6.9824e-01,
           -2.2560e-01,  1.2691e-01]]],


        [[[-1.3520e+00, -8.6254e-01,  6.4105e-01,  ...,  1.4209e+00,
            1.2808e+00, -2.0666e+00]],

         [[-1.5838e+00, -1.8343e-01,  1.4484e+00,  ...,  1.7485e+00,
            7.2797e-01, -1.7167e+00]],

         [[-8.3071e-01, -1.5321e+00,  2.6427e-01,  ...,  9.6555e-01,
           -3.2759e-02, -2.1295e+00]],

         ...,

         [[-5.1713e-01, -2.4522e-01,  2.6398e-01,  ..., -6.3095e-01,
           -6.8307e-01, -9.7277e-01]],

         [[-7.5159e-02, -1.9513e+00,  1.1293e+00,  ...,  1.4724e+00,
            1.2307e+00, -1.7710e+00]],

         [[-3.3498e-01, -2.4758e+00, -3.5716e-03,  ...,  1.1573e+00,
            4.7360e-01, -1.3803e+00]]]], device='cuda:0',
       grad_fn=<StackBackward>), tensor([[[0.6008, 0.0892, 0.9368],
         [0.7885, 0.7176, 0.2762],
         [0.5920, 0.5035, 0.2183],
         ...,
         [0.8477, 0.5957, 0.6450],
         [0.3734, 0.9142, 0.1975],
         [0.5921, 0.4923, 0.9228]]], device='cuda:0',
       grad_fn=<SigmoidBackward>), tensor([[[[0.6182, 0.0729, 0.9421],
          [0.7665, 0.6737, 0.2701],
          [0.5702, 0.4793, 0.2334],
          ...,
          [0.8549, 0.5399, 0.6282],
          [0.3855, 0.8926, 0.1973],
          [0.5657, 0.4334, 0.9253]]],


        [[[0.6246, 0.0635, 0.9467],
          [0.7679, 0.6362, 0.2820],
          [0.5828, 0.4212, 0.2609],
          ...,
          [0.8669, 0.5094, 0.6487],
          [0.4042, 0.8747, 0.2119],
          [0.5796, 0.3777, 0.9308]]],


        [[[0.6470, 0.0592, 0.9489],
          [0.8006, 0.6218, 0.2826],
          [0.6049, 0.3782, 0.2676],
          ...,
          [0.8814, 0.4771, 0.6599],
          [0.4326, 0.8574, 0.2088],
          [0.6012, 0.3398, 0.9287]]],


        [[[0.6486, 0.0548, 0.9529],
          [0.8019, 0.5991, 0.2802],
          [0.6127, 0.3419, 0.2870],
          ...,
          [0.8874, 0.4426, 0.6777],
          [0.4499, 0.8469, 0.2141],
          [0.6327, 0.3033, 0.9308]]],


        [[[0.6462, 0.0498, 0.9544],
          [0.8076, 0.5733, 0.2659],
          [0.6206, 0.2954, 0.3035],
          ...,
          [0.8951, 0.3960, 0.6813],
          [0.4728, 0.8146, 0.2194],
          [0.6613, 0.2495, 0.9296]]],


        [[[0.6409, 0.0401, 0.9551],
          [0.7970, 0.5259, 0.2534],
          [0.6139, 0.2472, 0.2880],
          ...,
          [0.8919, 0.3560, 0.6751],
          [0.4688, 0.7821, 0.2408],
          [0.6514, 0.1996, 0.9302]]]], device='cuda:0'))

 BEV embed: : tensor([[[ 0.1293, -0.8296,  0.7280,  ...,  1.6130,  0.2804, -0.1444]],

        [[ 1.3599,  0.1083,  0.3239,  ...,  1.5450, -0.0049,  0.6069]],

        [[-0.4854,  0.3193,  0.0033,  ...,  0.6170, -0.5052,  1.6112]],

        ...,

        [[-0.0569, -0.8779, -0.2223,  ...,  1.4433,  0.3969,  0.8107]],

        [[-0.4201,  0.0608, -0.6404,  ...,  1.1649,  0.0385,  0.7621]],

        [[-0.7017, -0.4087,  0.1059,  ...,  1.0952,  0.7742,  0.3575]]],
       device='cuda:0', grad_fn=<PermuteBackward>)

 BEV embed shape : torch.Size([2500, 1, 256])

 OUTS: : {'bev_embed': tensor([[[ 0.1293, -0.8296,  0.7280,  ...,  1.6130,  0.2804, -0.1444]],

        [[ 1.3599,  0.1083,  0.3239,  ...,  1.5450, -0.0049,  0.6069]],

        [[-0.4854,  0.3193,  0.0033,  ...,  0.6170, -0.5052,  1.6112]],

        ...,

        [[-0.0569, -0.8779, -0.2223,  ...,  1.4433,  0.3969,  0.8107]],

        [[-0.4201,  0.0608, -0.6404,  ...,  1.1649,  0.0385,  0.7621]],

        [[-0.7017, -0.4087,  0.1059,  ...,  1.0952,  0.7742,  0.3575]]],
       device='cuda:0', grad_fn=<PermuteBackward>), 'all_cls_scores': tensor([[[[-5.4572, -5.1893, -4.7428,  ..., -4.6594, -4.8788, -3.9993],
          [-5.6953, -4.7950, -5.2767,  ..., -4.7372, -4.8567, -3.8297],
          [-5.4498, -4.6806, -5.1218,  ..., -4.9149, -5.2901, -4.0318],
          ...,
          [-5.4810, -4.9584, -4.6668,  ..., -4.7741, -4.8124, -4.4239],
          [-5.4422, -4.8431, -4.6971,  ..., -4.6535, -4.7457, -4.1356],
          [-5.0897, -5.0017, -5.2579,  ..., -4.8480, -4.6128, -4.1525]]],


        [[[-5.6837, -4.8795, -4.6142,  ..., -4.6530, -5.0175, -4.0843],
          [-5.4638, -4.8939, -4.9185,  ..., -4.3867, -5.2435, -3.6732],
          [-4.8546, -4.8492, -4.8595,  ..., -4.3300, -5.2320, -4.1141],
          ...,
          [-5.2415, -4.6239, -4.5123,  ..., -4.7526, -4.9827, -4.1888],
          [-5.2019, -4.5921, -4.4769,  ..., -4.2098, -5.0358, -3.8371],
          [-5.0576, -4.8406, -4.8562,  ..., -4.4516, -5.0088, -4.4705]]],


        [[[-4.8678, -4.4699, -4.7979,  ..., -4.3247, -5.0595, -3.7088],
          [-4.6952, -4.4039, -4.7145,  ..., -4.4781, -5.0236, -3.6865],
          [-4.5921, -4.6356, -4.8614,  ..., -4.2458, -4.5733, -3.5416],
          ...,
          [-4.5869, -4.6236, -4.8718,  ..., -4.5788, -4.7500, -3.9882],
          [-4.5347, -4.3073, -4.6362,  ..., -4.6593, -4.8257, -3.8271],
          [-4.5200, -4.4576, -4.8892,  ..., -4.5548, -4.6515, -3.6957]]],


        [[[-4.7273, -4.7545, -4.9296,  ..., -4.4696, -4.9322, -4.2795],
          [-4.6115, -4.8509, -4.4234,  ..., -4.6401, -5.0719, -3.9757],
          [-4.4613, -4.5875, -4.7667,  ..., -4.1507, -4.7113, -4.1756],
          ...,
          [-4.6189, -4.7374, -4.5432,  ..., -4.8981, -5.0929, -4.1747],
          [-4.5261, -4.2549, -4.2248,  ..., -4.5313, -4.4248, -4.4811],
          [-4.4230, -4.5759, -4.6660,  ..., -4.3900, -4.6889, -4.2198]]],


        [[[-4.7943, -4.8909, -4.6225,  ..., -4.4196, -4.7818, -4.1378],
          [-4.7870, -4.9732, -4.4694,  ..., -4.4646, -4.5213, -4.0698],
          [-4.8918, -4.6510, -4.8131,  ..., -4.4246, -4.7774, -4.1090],
          ...,
          [-4.7150, -4.5008, -4.7804,  ..., -4.1868, -4.3526, -4.1594],
          [-4.8133, -4.3110, -4.4270,  ..., -4.5936, -4.3681, -4.4684],
          [-4.6942, -4.5433, -4.4132,  ..., -4.0754, -4.9429, -3.8678]]],


        [[[-4.8882, -4.5173, -4.7995,  ..., -4.5272, -4.3509, -4.1021],
          [-4.9781, -4.4935, -4.1460,  ..., -4.5855, -4.3026, -4.0717],
          [-4.7764, -4.0207, -4.9557,  ..., -4.4496, -4.1521, -3.9652],
          ...,
          [-5.1013, -3.7736, -4.7318,  ..., -4.1729, -4.4965, -3.9808],
          [-5.0448, -4.3637, -4.2885,  ..., -4.7188, -4.4574, -4.2272],
          [-4.7235, -4.1635, -4.3599,  ..., -4.3471, -4.3950, -3.9126]]]],
       device='cuda:0', grad_fn=<StackBackward>), 'all_bbox_preds': tensor([[[[ 1.2107e+01, -4.3730e+01,  1.7089e-02,  ..., -9.9746e-02,
           -2.4171e-02, -5.6924e-02],
          [ 2.7293e+01,  1.7787e+01,  7.5582e-02,  ..., -1.9603e-01,
            2.1632e-02, -8.2514e-02],
          [ 7.1903e+00, -2.1232e+00,  3.2717e-02,  ..., -1.5703e-01,
            4.3668e-03, -1.6388e-02],
          ...,
          [ 3.6345e+01,  4.0844e+00,  4.5601e-03,  ..., -1.1709e-01,
            3.5676e-02, -7.9955e-02],
          [-1.1729e+01,  4.0202e+01,  5.4608e-02,  ..., -7.6952e-02,
            6.1577e-03,  2.7925e-03],
          [ 6.7271e+00, -6.8151e+00,  4.0632e-02,  ..., -8.8106e-03,
            3.0187e-02, -3.3621e-02]]],


        [[[ 1.2761e+01, -4.4699e+01,  4.2132e-02,  ..., -1.3769e-01,
            1.2360e-01, -1.8943e-02],
          [ 2.7435e+01,  1.3949e+01,  7.0598e-02,  ..., -8.2452e-02,
            2.4966e-02,  2.6675e-02],
          [ 8.4783e+00, -8.0718e+00,  4.4756e-02,  ..., -1.1741e-01,
            5.5148e-02,  6.6359e-02],
          ...,
          [ 3.7574e+01,  9.5883e-01,  5.3295e-02,  ..., -1.0619e-01,
           -3.9244e-02,  2.4133e-03],
          [-9.8140e+00,  3.8369e+01,  1.0304e-01,  ..., -5.1650e-02,
            4.5338e-02,  9.5519e-02],
          [ 8.1467e+00, -1.2526e+01,  4.5956e-02,  ..., -5.6333e-02,
            8.8331e-03, -9.8498e-04]]],


        [[[ 1.5049e+01, -4.5143e+01,  5.5305e-02,  ..., -1.0463e-01,
            9.2532e-02,  1.3399e-02],
          [ 3.0780e+01,  1.2477e+01,  1.0468e-01,  ..., -1.7094e-01,
            9.9460e-02,  1.5045e-02],
          [ 1.0744e+01, -1.2475e+01, -7.0503e-03,  ..., -1.4262e-01,
            1.0456e-01, -7.2272e-02],
          ...,
          [ 3.9057e+01, -2.3484e+00,  1.6039e-01,  ..., -7.8494e-02,
            1.2747e-02, -7.2072e-02],
          [-6.9030e+00,  3.6599e+01,  6.9725e-02,  ..., -6.8338e-02,
            4.8767e-02, -1.6617e-02],
          [ 1.0367e+01, -1.6409e+01,  1.2179e-01,  ..., -8.8199e-02,
            2.9149e-02,  1.6142e-02]]],


        [[[ 1.5221e+01, -4.5593e+01, -2.0066e-02,  ..., -8.7899e-03,
           -1.0365e-01, -9.3391e-02],
          [ 3.0910e+01,  1.0147e+01,  8.8128e-02,  ..., -2.7153e-02,
            4.5605e-04, -8.3628e-02],
          [ 1.1541e+01, -1.6192e+01, -4.3834e-02,  ..., -3.8460e-02,
            7.3393e-02, -4.3180e-02],
          ...,
          [ 3.9667e+01, -5.8798e+00,  1.4183e-01,  ...,  1.3041e-01,
            1.0334e-01, -6.2536e-02],
          [-5.1321e+00,  3.5526e+01,  2.1402e-02,  ...,  2.5420e-02,
            7.3551e-02, -1.2177e-01],
          [ 1.3584e+01, -2.0138e+01,  5.1912e-02,  ...,  1.8380e-02,
           -3.5020e-02, -7.1631e-02]]],


        [[[ 1.4967e+01, -4.6097e+01,  8.4092e-03,  ..., -2.4388e-02,
           -1.3859e-03, -2.5465e-02],
          [ 3.1497e+01,  7.5054e+00,  3.5182e-02,  ..., -5.5260e-03,
            2.0576e-02, -1.1522e-01],
          [ 1.2353e+01, -2.0954e+01,  6.9128e-02,  ..., -1.2914e-02,
            3.8026e-02, -4.5879e-02],
          ...,
          [ 4.0460e+01, -1.0652e+01,  1.2515e-01,  ...,  6.6260e-02,
            9.7692e-02, -9.7202e-02],
          [-2.7878e+00,  3.2211e+01,  1.1871e-02,  ..., -6.7858e-02,
            1.1857e-01, -3.8602e-02],
          [ 1.6514e+01, -2.5647e+01,  6.3980e-02,  ...,  3.3143e-02,
            9.4786e-02, -4.9758e-02]]],


        [[[ 1.4426e+01, -4.7097e+01, -5.2460e-03,  ..., -6.6539e-02,
            4.3945e-02,  2.6836e-02],
          [ 3.0412e+01,  2.6558e+00,  1.2046e-01,  ..., -8.9246e-03,
           -4.9321e-03, -6.6688e-02],
          [ 1.1659e+01, -2.5888e+01, -1.8310e-02,  ..., -8.6312e-02,
            8.2378e-02, -7.3443e-02],
          ...,
          [ 4.0136e+01, -1.4745e+01,  1.2514e-01,  ...,  5.3483e-02,
            5.7152e-02, -7.0049e-02],
          [-3.1943e+00,  2.8882e+01,  1.2418e-02,  ..., -6.5490e-02,
            1.1341e-01,  6.0559e-04],
          [ 1.5504e+01, -3.0764e+01,  3.6519e-02,  ...,  8.1577e-03,
            1.5099e-01, -7.5304e-03]]]], device='cuda:0',
       grad_fn=<StackBackward>), 'enc_cls_scores': None, 'enc_bbox_preds': None}

 all_cs_scores shape : torch.Size([6, 1, 900, 10])

 all_bbox_preds shape : torch.Size([6, 1, 900, 10])

 ENTERED BEVFORMER HEAD INIT : 

 ENTERED BEVFORMER HEAD INIT_LAYERS : 

 Printing inside the BEVFormer_Head _init_layers______________________________________________ : 

 CLS branch: : [Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=10, bias=True)]

 REG branch: : Sequential(
  (0): Linear(in_features=256, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): ReLU()
  (4): Linear(in_features=256, out_features=10, bias=True)
)

 Printing inside the BEVFormer_Head init___________________________________________________ : 

 __dict__ of BEVFormer Head : {'bev_h': 50, 'bev_w': 50, 'fp16_enabled': False, 'with_box_refine': True, 'as_two_stage': False, 'code_size': 10, 'bbox_coder': <projects.mmdet3d_plugin.core.bbox.coders.nms_free_coder.NMSFreeCoder object at 0x7f97622108e0>, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'real_w': 102.4, 'real_h': 102.4, 'num_cls_fcs': 1, 'training': True, '_parameters': OrderedDict([('code_weights', Parameter containing:
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000,
        0.2000]))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict([('loss_cls', FocalLoss()), ('loss_bbox', L1Loss()), ('loss_iou', GIoULoss()), ('activate', ReLU(inplace=True)), ('positional_encoding', LearnedPositionalEncoding(num_feats=128, row_num_embed=50, col_num_embed=50)), ('transformer', PerceptionTransformer(
  (encoder): BEVFormerEncoder(
    (layers): ModuleList(
      (0): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): DetectionTransformerDecoder(
    (layers): ModuleList(
      (0): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (can_bus_mlp): Sequential(
    (0): Linear(in_features=18, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=256, bias=True)
    (3): ReLU(inplace=True)
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)), ('cls_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('reg_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('bev_embedding', Embedding(2500, 256)), ('query_embedding', Embedding(900, 512))]), '_is_init': False, 'init_cfg': None, 'bg_cls_weight': 0, 'sync_cls_avg_factor': True, 'num_query': 900, 'num_classes': 10, 'in_channels': 256, 'num_reg_fcs': 2, 'train_cfg': None, 'test_cfg': None, 'cls_out_channels': 10, 'act_cfg': {'type': 'ReLU', 'inplace': True}, 'embed_dims': 256}

 ENTERED BEVFORMER HEAD INIT : 

 ENTERED BEVFORMER HEAD INIT_LAYERS : 

 Printing inside the BEVFormer_Head _init_layers______________________________________________ : 

 CLS branch: : [Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=10, bias=True)]

 REG branch: : Sequential(
  (0): Linear(in_features=256, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): ReLU()
  (4): Linear(in_features=256, out_features=10, bias=True)
)

 Printing inside the BEVFormer_Head init___________________________________________________ : 

 __dict__ of BEVFormer Head : {'bev_h': 50, 'bev_w': 50, 'fp16_enabled': False, 'with_box_refine': True, 'as_two_stage': False, 'code_size': 10, 'bbox_coder': <projects.mmdet3d_plugin.core.bbox.coders.nms_free_coder.NMSFreeCoder object at 0x7f98b26127f0>, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'real_w': 102.4, 'real_h': 102.4, 'num_cls_fcs': 1, 'training': True, '_parameters': OrderedDict([('code_weights', Parameter containing:
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000,
        0.2000]))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict([('loss_cls', FocalLoss()), ('loss_bbox', L1Loss()), ('loss_iou', GIoULoss()), ('activate', ReLU(inplace=True)), ('positional_encoding', LearnedPositionalEncoding(num_feats=128, row_num_embed=50, col_num_embed=50)), ('transformer', PerceptionTransformer(
  (encoder): BEVFormerEncoder(
    (layers): ModuleList(
      (0): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): DetectionTransformerDecoder(
    (layers): ModuleList(
      (0): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (can_bus_mlp): Sequential(
    (0): Linear(in_features=18, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=256, bias=True)
    (3): ReLU(inplace=True)
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)), ('cls_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('reg_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('bev_embedding', Embedding(2500, 256)), ('query_embedding', Embedding(900, 512))]), '_is_init': False, 'init_cfg': None, 'bg_cls_weight': 0, 'sync_cls_avg_factor': True, 'num_query': 900, 'num_classes': 10, 'in_channels': 256, 'num_reg_fcs': 2, 'train_cfg': None, 'test_cfg': None, 'cls_out_channels': 10, 'act_cfg': {'type': 'ReLU', 'inplace': True}, 'embed_dims': 256}

 ENTERED BEVFORMER HEAD INIT : 

 ENTERED BEVFORMER HEAD INIT_LAYERS : 

 Printing inside the BEVFormer_Head _init_layers______________________________________________ : 

 CLS branch: : [Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=10, bias=True)]

 REG branch: : Sequential(
  (0): Linear(in_features=256, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): ReLU()
  (4): Linear(in_features=256, out_features=10, bias=True)
)

 Printing inside the BEVFormer_Head init___________________________________________________ : 

 __dict__ of BEVFormer Head : {'bev_h': 50, 'bev_w': 50, 'fp16_enabled': False, 'with_box_refine': True, 'as_two_stage': False, 'code_size': 10, 'bbox_coder': <projects.mmdet3d_plugin.core.bbox.coders.nms_free_coder.NMSFreeCoder object at 0x7fc5d5d317f0>, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'real_w': 102.4, 'real_h': 102.4, 'num_cls_fcs': 1, 'training': True, '_parameters': OrderedDict([('code_weights', Parameter containing:
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000,
        0.2000]))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict([('loss_cls', FocalLoss()), ('loss_bbox', L1Loss()), ('loss_iou', GIoULoss()), ('activate', ReLU(inplace=True)), ('positional_encoding', LearnedPositionalEncoding(num_feats=128, row_num_embed=50, col_num_embed=50)), ('transformer', PerceptionTransformer(
  (encoder): BEVFormerEncoder(
    (layers): ModuleList(
      (0): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): DetectionTransformerDecoder(
    (layers): ModuleList(
      (0): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (can_bus_mlp): Sequential(
    (0): Linear(in_features=18, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=256, bias=True)
    (3): ReLU(inplace=True)
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)), ('cls_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('reg_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('bev_embedding', Embedding(2500, 256)), ('query_embedding', Embedding(900, 512))]), '_is_init': False, 'init_cfg': None, 'bg_cls_weight': 0, 'sync_cls_avg_factor': True, 'num_query': 900, 'num_classes': 10, 'in_channels': 256, 'num_reg_fcs': 2, 'train_cfg': None, 'test_cfg': None, 'cls_out_channels': 10, 'act_cfg': {'type': 'ReLU', 'inplace': True}, 'embed_dims': 256}

 ENTERED BEVFORMER HEAD INIT : 

 ENTERED BEVFORMER HEAD INIT_LAYERS : 

 Printing inside the BEVFormer_Head _init_layers______________________________________________ : 

 CLS branch: : [Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=10, bias=True)]

 REG branch: : Sequential(
  (0): Linear(in_features=256, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): ReLU()
  (4): Linear(in_features=256, out_features=10, bias=True)
)

 Printing inside the BEVFormer_Head init___________________________________________________ : 

 __dict__ of BEVFormer Head : {'bev_h': 50, 'bev_w': 50, 'fp16_enabled': False, 'with_box_refine': True, 'as_two_stage': False, 'code_size': 10, 'bbox_coder': <projects.mmdet3d_plugin.core.bbox.coders.nms_free_coder.NMSFreeCoder object at 0x7f045d194a00>, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'real_w': 102.4, 'real_h': 102.4, 'num_cls_fcs': 1, 'training': True, '_parameters': OrderedDict([('code_weights', Parameter containing:
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000,
        0.2000]))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict([('loss_cls', FocalLoss()), ('loss_bbox', L1Loss()), ('loss_iou', GIoULoss()), ('activate', ReLU(inplace=True)), ('positional_encoding', LearnedPositionalEncoding(num_feats=128, row_num_embed=50, col_num_embed=50)), ('transformer', PerceptionTransformer(
  (encoder): BEVFormerEncoder(
    (layers): ModuleList(
      (0): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): DetectionTransformerDecoder(
    (layers): ModuleList(
      (0): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (can_bus_mlp): Sequential(
    (0): Linear(in_features=18, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=256, bias=True)
    (3): ReLU(inplace=True)
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)), ('cls_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('reg_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('bev_embedding', Embedding(2500, 256)), ('query_embedding', Embedding(900, 512))]), '_is_init': False, 'init_cfg': None, 'bg_cls_weight': 0, 'sync_cls_avg_factor': True, 'num_query': 900, 'num_classes': 10, 'in_channels': 256, 'num_reg_fcs': 2, 'train_cfg': None, 'test_cfg': None, 'cls_out_channels': 10, 'act_cfg': {'type': 'ReLU', 'inplace': True}, 'embed_dims': 256}

 ENTERED BEVFORMER HEAD INIT : 

 ENTERED BEVFORMER HEAD INIT_LAYERS : 

 Printing inside the BEVFormer_Head _init_layers______________________________________________ : 

 CLS branch: : [Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=10, bias=True)]

 REG branch: : Sequential(
  (0): Linear(in_features=256, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): ReLU()
  (4): Linear(in_features=256, out_features=10, bias=True)
)

 Printing inside the BEVFormer_Head init___________________________________________________ : 

 __dict__ of BEVFormer Head : {'bev_h': 50, 'bev_w': 50, 'fp16_enabled': False, 'with_box_refine': True, 'as_two_stage': False, 'code_size': 10, 'bbox_coder': <projects.mmdet3d_plugin.core.bbox.coders.nms_free_coder.NMSFreeCoder object at 0x7f5c41d75a00>, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'real_w': 102.4, 'real_h': 102.4, 'num_cls_fcs': 1, 'training': True, '_parameters': OrderedDict([('code_weights', Parameter containing:
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000,
        0.2000]))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict([('loss_cls', FocalLoss()), ('loss_bbox', L1Loss()), ('loss_iou', GIoULoss()), ('activate', ReLU(inplace=True)), ('positional_encoding', LearnedPositionalEncoding(num_feats=128, row_num_embed=50, col_num_embed=50)), ('transformer', PerceptionTransformer(
  (encoder): BEVFormerEncoder(
    (layers): ModuleList(
      (0): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): DetectionTransformerDecoder(
    (layers): ModuleList(
      (0): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (can_bus_mlp): Sequential(
    (0): Linear(in_features=18, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=256, bias=True)
    (3): ReLU(inplace=True)
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)), ('cls_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('reg_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('bev_embedding', Embedding(2500, 256)), ('query_embedding', Embedding(900, 512))]), '_is_init': False, 'init_cfg': None, 'bg_cls_weight': 0, 'sync_cls_avg_factor': True, 'num_query': 900, 'num_classes': 10, 'in_channels': 256, 'num_reg_fcs': 2, 'train_cfg': None, 'test_cfg': None, 'cls_out_channels': 10, 'act_cfg': {'type': 'ReLU', 'inplace': True}, 'embed_dims': 256}

 ENTERED BEVFORMER HEAD INIT : 

 ENTERED BEVFORMER HEAD INIT_LAYERS : 

 Printing inside the BEVFormer_Head _init_layers______________________________________________ : 

 CLS branch: : [Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=10, bias=True)]

 REG branch: : Sequential(
  (0): Linear(in_features=256, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): ReLU()
  (4): Linear(in_features=256, out_features=10, bias=True)
)

 Printing inside the BEVFormer_Head init___________________________________________________ : 

 __dict__ of BEVFormer Head : {'bev_h': 50, 'bev_w': 50, 'fp16_enabled': False, 'with_box_refine': True, 'as_two_stage': False, 'code_size': 10, 'bbox_coder': <projects.mmdet3d_plugin.core.bbox.coders.nms_free_coder.NMSFreeCoder object at 0x7f1783c84940>, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'real_w': 102.4, 'real_h': 102.4, 'num_cls_fcs': 1, 'training': True, '_parameters': OrderedDict([('code_weights', Parameter containing:
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000,
        0.2000]))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict([('loss_cls', FocalLoss()), ('loss_bbox', L1Loss()), ('loss_iou', GIoULoss()), ('activate', ReLU(inplace=True)), ('positional_encoding', LearnedPositionalEncoding(num_feats=128, row_num_embed=50, col_num_embed=50)), ('transformer', PerceptionTransformer(
  (encoder): BEVFormerEncoder(
    (layers): ModuleList(
      (0): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): DetectionTransformerDecoder(
    (layers): ModuleList(
      (0): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (can_bus_mlp): Sequential(
    (0): Linear(in_features=18, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=256, bias=True)
    (3): ReLU(inplace=True)
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)), ('cls_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('reg_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('bev_embedding', Embedding(2500, 256)), ('query_embedding', Embedding(900, 512))]), '_is_init': False, 'init_cfg': None, 'bg_cls_weight': 0, 'sync_cls_avg_factor': True, 'num_query': 900, 'num_classes': 10, 'in_channels': 256, 'num_reg_fcs': 2, 'train_cfg': None, 'test_cfg': None, 'cls_out_channels': 10, 'act_cfg': {'type': 'ReLU', 'inplace': True}, 'embed_dims': 256}

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 Printing inside the BEVFormer_Head forward___________________________________________________ : 

 Query embedding: : Embedding(900, 512)

 Object query embeds: : Parameter containing:
tensor([[-0.4259, -1.3660, -0.2644,  ...,  0.0852, -0.1509,  0.4264],
        [ 0.4416,  0.4936,  0.5256,  ..., -0.2080,  1.5487, -0.7089],
        [ 1.1582, -0.1082, -1.1008,  ..., -0.0726, -1.0095, -0.6947],
        ...,
        [ 0.8768, -0.1981,  1.1983,  ..., -0.0531,  1.1525, -0.2061],
        [ 0.2863,  0.9710,  0.6123,  ..., -0.8295,  1.3894,  0.1734],
        [ 0.7548,  1.0330,  1.8136,  ..., -0.3277,  0.6175,  0.8807]],
       device='cuda:0')

 Object query embeds shape: torch.Size([900, 512]) : 

 BEV Qs: : Parameter containing:
tensor([[-0.7539, -0.8606, -1.5688,  ...,  0.5348, -0.3786, -1.7318],
        [ 0.0436,  0.1424, -0.1229,  ..., -0.7287,  1.0243, -0.1771],
        [ 0.7314,  1.1423,  0.2060,  ...,  0.9336,  1.3491,  0.1780],
        ...,
        [-1.4946,  1.5187, -0.4492,  ...,  0.6383,  0.9608,  0.8085],
        [ 0.2285,  0.2309,  0.9725,  ...,  0.2353, -0.1845,  0.0686],
        [-0.1813,  0.0787, -0.1963,  ..., -0.6626,  0.8098,  0.0484]],
       device='cuda:0')

 BEV Qs shape: torch.Size([2500, 256]) : 

 BEV mask: : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')

 BEV mask shape: torch.Size([1, 50, 50]) : 

 BEV pos: : tensor([[[[ 1.1632, -0.5398,  2.4410,  ..., -1.1669, -1.2498,  1.3227],
          [ 1.1632, -0.5398,  2.4410,  ..., -1.1669, -1.2498,  1.3227],
          [ 1.1632, -0.5398,  2.4410,  ..., -1.1669, -1.2498,  1.3227],
          ...,
          [ 1.1632, -0.5398,  2.4410,  ..., -1.1669, -1.2498,  1.3227],
          [ 1.1632, -0.5398,  2.4410,  ..., -1.1669, -1.2498,  1.3227],
          [ 1.1632, -0.5398,  2.4410,  ..., -1.1669, -1.2498,  1.3227]],

         [[ 0.2218,  0.2276, -0.7997,  ..., -0.3673, -0.4338,  0.4864],
          [ 0.2218,  0.2276, -0.7997,  ..., -0.3673, -0.4338,  0.4864],
          [ 0.2218,  0.2276, -0.7997,  ..., -0.3673, -0.4338,  0.4864],
          ...,
          [ 0.2218,  0.2276, -0.7997,  ..., -0.3673, -0.4338,  0.4864],
          [ 0.2218,  0.2276, -0.7997,  ..., -0.3673, -0.4338,  0.4864],
          [ 0.2218,  0.2276, -0.7997,  ..., -0.3673, -0.4338,  0.4864]],

         [[-0.9746, -0.9412,  1.8879,  ..., -0.6675,  0.2121, -1.7216],
          [-0.9746, -0.9412,  1.8879,  ..., -0.6675,  0.2121, -1.7216],
          [-0.9746, -0.9412,  1.8879,  ..., -0.6675,  0.2121, -1.7216],
          ...,
          [-0.9746, -0.9412,  1.8879,  ..., -0.6675,  0.2121, -1.7216],
          [-0.9746, -0.9412,  1.8879,  ..., -0.6675,  0.2121, -1.7216],
          [-0.9746, -0.9412,  1.8879,  ..., -0.6675,  0.2121, -1.7216]],

         ...,

         [[ 1.0568,  1.0568,  1.0568,  ...,  1.0568,  1.0568,  1.0568],
          [ 0.2385,  0.2385,  0.2385,  ...,  0.2385,  0.2385,  0.2385],
          [ 0.6219,  0.6219,  0.6219,  ...,  0.6219,  0.6219,  0.6219],
          ...,
          [ 0.2538,  0.2538,  0.2538,  ...,  0.2538,  0.2538,  0.2538],
          [-0.2278, -0.2278, -0.2278,  ..., -0.2278, -0.2278, -0.2278],
          [-0.7648, -0.7648, -0.7648,  ..., -0.7648, -0.7648, -0.7648]],

         [[ 0.9530,  0.9530,  0.9530,  ...,  0.9530,  0.9530,  0.9530],
          [ 1.8456,  1.8456,  1.8456,  ...,  1.8456,  1.8456,  1.8456],
          [-0.2594, -0.2594, -0.2594,  ..., -0.2594, -0.2594, -0.2594],
          ...,
          [-0.2839, -0.2839, -0.2839,  ..., -0.2839, -0.2839, -0.2839],
          [ 0.0471,  0.0471,  0.0471,  ...,  0.0471,  0.0471,  0.0471],
          [ 0.5158,  0.5158,  0.5158,  ...,  0.5158,  0.5158,  0.5158]],

         [[ 1.3177,  1.3177,  1.3177,  ...,  1.3177,  1.3177,  1.3177],
          [ 0.3279,  0.3279,  0.3279,  ...,  0.3279,  0.3279,  0.3279],
          [-0.2635, -0.2635, -0.2635,  ..., -0.2635, -0.2635, -0.2635],
          ...,
          [ 0.0324,  0.0324,  0.0324,  ...,  0.0324,  0.0324,  0.0324],
          [-0.1789, -0.1789, -0.1789,  ..., -0.1789, -0.1789, -0.1789],
          [-0.5444, -0.5444, -0.5444,  ..., -0.5444, -0.5444, -0.5444]]]],
       device='cuda:0')

 BEV pos shape: torch.Size([1, 256, 50, 50]) : 

 OUTPUTS : (tensor([[[-0.0150, -0.8553, -1.1990,  ...,  0.7575,  0.1687,  0.7072]],

        [[-0.4939, -1.0361,  0.1543,  ...,  0.2843, -0.2015,  1.4339]],

        [[ 0.7048, -0.0066, -0.3703,  ...,  0.5556, -0.8595,  0.3515]],

        ...,

        [[-0.9080,  0.5076, -1.2482,  ...,  0.3816, -0.0966,  1.8923]],

        [[-0.3948,  1.2642, -1.5466,  ...,  0.2629, -0.0686,  1.8755]],

        [[-0.2498,  0.9610, -1.6961,  ...,  0.7198,  0.4534,  1.3083]]],
       device='cuda:0'), tensor([[[[ 4.7493e-01, -6.3118e-01, -6.2072e-01,  ...,  3.3914e-01,
            1.0223e+00,  1.1651e+00]],

         [[ 7.3860e-01, -8.6462e-01, -2.0635e-01,  ...,  1.0955e+00,
            1.9080e-01,  1.2571e+00]],

         [[ 3.3464e-01,  9.4736e-01,  6.4085e-01,  ..., -1.5875e+00,
           -1.6582e+00,  8.4366e-01]],

         ...,

         [[ 6.2780e-01,  4.5882e-02, -4.2848e-01,  ...,  7.7657e-01,
           -8.5142e-02,  1.2424e+00]],

         [[-6.5721e-01, -9.6801e-02, -7.6842e-01,  ...,  5.7531e-01,
            9.6281e-01, -1.6316e+00]],

         [[-3.7371e-02,  2.2115e-02, -4.0225e-01,  ...,  1.0086e+00,
           -4.3000e-01,  1.1375e+00]]],


        [[[-3.6657e-01, -7.7050e-01, -7.1805e-02,  ...,  5.6593e-01,
            9.0628e-01,  9.1757e-01]],

         [[ 5.7526e-01, -9.8373e-01, -3.3383e-01,  ...,  1.1585e+00,
           -2.0209e-01,  8.2596e-01]],

         [[ 8.4051e-02,  6.0527e-01,  4.8148e-01,  ..., -5.5770e-01,
           -1.2042e+00,  1.2454e+00]],

         ...,

         [[ 2.9195e-01, -8.5794e-02,  9.3815e-02,  ...,  4.3499e-01,
            2.2433e-01,  1.0190e+00]],

         [[ 5.5069e-01,  1.4943e+00, -1.9225e+00,  ...,  8.6193e-01,
            6.0881e-01, -5.6247e-01]],

         [[ 5.3714e-01, -6.0854e-01,  1.2759e-01,  ...,  8.4181e-01,
            1.4830e-01,  1.6292e+00]]],


        [[[-1.3329e-01, -4.8685e-01,  1.8920e-01,  ..., -1.0744e-01,
           -2.1317e-01,  2.8969e-01]],

         [[-1.0145e-01, -5.8759e-01, -5.8767e-01,  ...,  4.5616e-01,
           -2.0716e-01,  4.6098e-01]],

         [[-3.5325e-01,  3.4917e-01, -2.6598e-01,  ..., -4.8799e-01,
           -1.0891e+00,  5.4675e-01]],

         ...,

         [[-1.7044e-01,  2.0682e-01,  6.4141e-02,  ..., -1.2768e-01,
           -4.1059e-01,  1.0573e+00]],

         [[ 4.0471e-01,  9.0340e-01, -1.2074e+00,  ...,  1.3385e-02,
            2.5148e-01, -9.1551e-01]],

         [[-8.5829e-02, -3.7056e-01, -2.5581e-01,  ...,  1.6368e-01,
           -5.4630e-01,  1.0257e+00]]],


        [[[-2.3538e-01, -7.6717e-01, -6.0727e-01,  ...,  5.3258e-01,
            5.1310e-01,  1.5484e+00]],

         [[ 2.9458e-01, -9.6331e-02, -7.6434e-02,  ...,  6.1454e-01,
           -1.0283e+00,  6.1503e-01]],

         [[ 1.6046e+00,  6.1421e-01,  5.9664e-01,  ..., -1.0276e+00,
           -1.0628e+00,  6.4369e-01]],

         ...,

         [[-3.4953e-01,  2.8513e-01, -4.7396e-01,  ...,  2.5388e-01,
           -1.2429e-01,  1.2122e+00]],

         [[ 1.7684e+00,  1.4054e-01,  1.0338e-01,  ...,  3.9049e-01,
           -8.7542e-01, -2.7632e-01]],

         [[ 7.0888e-01,  3.6670e-01,  2.9047e-01,  ...,  1.3617e-03,
           -7.1848e-01,  9.3709e-01]]],


        [[[ 6.1966e-01, -5.4492e-01, -5.3529e-01,  ...,  4.8466e-02,
            1.3787e-01,  1.2385e+00]],

         [[ 1.1423e+00,  7.5837e-01,  1.8933e-01,  ...,  6.4011e-01,
           -2.2086e+00,  3.6186e-01]],

         [[ 2.3777e+00,  4.5612e-01,  1.4918e+00,  ..., -3.8574e-01,
           -1.0371e+00,  4.2698e-01]],

         ...,

         [[ 2.6233e-01, -1.9726e-01,  2.4389e-01,  ...,  4.8616e-01,
           -5.1755e-01,  6.1288e-01]],

         [[ 1.3305e+00, -7.5342e-01,  3.1833e-01,  ..., -2.6318e-02,
           -6.0137e-01, -2.1192e-01]],

         [[ 1.5201e+00,  1.3634e+00,  8.6211e-01,  ...,  6.7038e-01,
           -1.6331e+00,  3.1456e-01]]],


        [[[ 1.2249e+00,  5.0415e-01,  1.1122e+00,  ..., -5.2566e-01,
            3.7039e-01,  1.5483e+00]],

         [[ 1.1166e+00,  1.4003e+00,  1.5382e+00,  ..., -4.6390e-02,
           -1.3244e+00,  1.4497e-01]],

         [[ 2.4985e+00,  1.6306e+00,  1.1712e+00,  ..., -5.1158e-01,
           -1.0515e+00,  2.1141e-01]],

         ...,

         [[ 1.1151e+00,  7.8094e-01,  1.2293e+00,  ...,  9.1122e-02,
           -3.8359e-01,  5.1181e-01]],

         [[ 1.4704e+00, -9.3985e-01,  3.8894e-01,  ..., -6.7259e-01,
           -5.5329e-01, -1.4976e-01]],

         [[ 1.5557e+00,  2.3212e+00,  1.3051e+00,  ...,  8.4053e-01,
           -8.9806e-01,  3.7325e-01]]]], device='cuda:0'), tensor([[[0.6264, 0.0600, 0.3168],
         [0.6620, 0.5795, 0.5226],
         [0.4859, 0.5455, 0.5059],
         ...,
         [0.6724, 0.7027, 0.5622],
         [0.3057, 0.9592, 0.6979],
         [0.5928, 0.5281, 0.5059]]], device='cuda:0'), tensor([[[[0.6200, 0.0605, 0.3189],
          [0.6709, 0.5678, 0.4955],
          [0.4752, 0.5278, 0.5297],
          ...,
          [0.6766, 0.6885, 0.5911],
          [0.3133, 0.9555, 0.9605],
          [0.5957, 0.5259, 0.4794]]],


        [[[0.6214, 0.0603, 0.3192],
          [0.6734, 0.5604, 0.4952],
          [0.4714, 0.5210, 0.5273],
          ...,
          [0.6768, 0.6879, 0.5925],
          [0.3043, 0.9547, 0.9600],
          [0.5973, 0.5231, 0.4746]]],


        [[[0.6201, 0.0593, 0.3209],
          [0.6706, 0.5566, 0.4941],
          [0.4693, 0.5158, 0.5265],
          ...,
          [0.6740, 0.6879, 0.5970],
          [0.3029, 0.9550, 0.9604],
          [0.5984, 0.5238, 0.4735]]],


        [[[0.6185, 0.0596, 0.3244],
          [0.6694, 0.5576, 0.4986],
          [0.4681, 0.5158, 0.5250],
          ...,
          [0.6734, 0.6913, 0.5988],
          [0.3076, 0.9551, 0.9603],
          [0.5965, 0.5262, 0.4763]]],


        [[[0.6154, 0.0592, 0.3218],
          [0.6652, 0.5556, 0.4961],
          [0.4648, 0.5175, 0.5220],
          ...,
          [0.6739, 0.6902, 0.6001],
          [0.3079, 0.9554, 0.9601],
          [0.5932, 0.5253, 0.4732]]],


        [[[0.6154, 0.0591, 0.3216],
          [0.6665, 0.5540, 0.4952],
          [0.4660, 0.5183, 0.5207],
          ...,
          [0.6735, 0.6908, 0.5984],
          [0.3071, 0.9559, 0.9599],
          [0.5927, 0.5228, 0.4724]]]], device='cuda:0'))

 BEV embed: : tensor([[[-0.0150, -0.8553, -1.1990,  ...,  0.7575,  0.1687,  0.7072]],

        [[-0.4939, -1.0361,  0.1543,  ...,  0.2843, -0.2015,  1.4339]],

        [[ 0.7048, -0.0066, -0.3703,  ...,  0.5556, -0.8595,  0.3515]],

        ...,

        [[-0.9080,  0.5076, -1.2482,  ...,  0.3816, -0.0966,  1.8923]],

        [[-0.3948,  1.2642, -1.5466,  ...,  0.2629, -0.0686,  1.8755]],

        [[-0.2498,  0.9610, -1.6961,  ...,  0.7198,  0.4534,  1.3083]]],
       device='cuda:0')

 BEV embed shape : torch.Size([2500, 1, 256])

 OUTS: : {'bev_embed': tensor([[[-0.0150, -0.8553, -1.1990,  ...,  0.7575,  0.1687,  0.7072]],

        [[-0.4939, -1.0361,  0.1543,  ...,  0.2843, -0.2015,  1.4339]],

        [[ 0.7048, -0.0066, -0.3703,  ...,  0.5556, -0.8595,  0.3515]],

        ...,

        [[-0.9080,  0.5076, -1.2482,  ...,  0.3816, -0.0966,  1.8923]],

        [[-0.3948,  1.2642, -1.5466,  ...,  0.2629, -0.0686,  1.8755]],

        [[-0.2498,  0.9610, -1.6961,  ...,  0.7198,  0.4534,  1.3083]]],
       device='cuda:0'), 'all_cls_scores': tensor([[[[ -8.2085,  -8.4644,  -8.8022,  ...,  -6.0075,  -4.5761,  -6.1691],
          [ -8.8877,  -9.2217,  -9.5429,  ...,  -6.2708,  -6.1937,  -5.7140],
          [ -6.9622,  -8.5855,  -8.2246,  ...,  -5.2463,  -3.5725,  -5.0271],
          ...,
          [ -7.8812,  -8.8304,  -8.5139,  ...,  -5.3237,  -4.0690,  -5.3406],
          [ -4.7783,  -5.1506,  -7.4647,  ...,  -5.5716,  -2.3811,  -4.5333],
          [ -9.5609,  -8.6000,  -9.2565,  ...,  -6.6051,  -6.6669,  -4.4036]]],


        [[[ -6.8274,  -6.8371,  -8.1822,  ...,  -5.4881,  -4.2399,  -5.7050],
          [ -7.1805,  -7.8710,  -9.8032,  ...,  -7.0194,  -6.9679,  -6.1489],
          [ -7.3065,  -8.4081,  -8.0249,  ...,  -5.1207,  -4.6788,  -5.0939],
          ...,
          [ -6.9748,  -7.3588,  -8.2003,  ...,  -5.2473,  -4.0738,  -5.2266],
          [ -5.7626,  -5.9866,  -7.6326,  ...,  -4.8358,  -2.4980,  -3.9432],
          [ -7.8931,  -8.8407,  -9.5705,  ...,  -7.5791,  -9.1503,  -5.2522]]],


        [[[ -6.3640,  -6.4446,  -7.6847,  ...,  -5.2202,  -4.3079,  -6.0328],
          [ -7.6240,  -7.8422,  -9.2080,  ...,  -6.3130,  -5.9222,  -6.0810],
          [ -6.6560,  -7.4179,  -7.7007,  ...,  -4.6452,  -4.3538,  -4.7781],
          ...,
          [ -6.3253,  -6.9952,  -7.8186,  ...,  -5.4399,  -4.2408,  -5.4065],
          [ -6.0029,  -6.4140,  -7.5818,  ...,  -5.2450,  -2.4199,  -4.4090],
          [ -8.5878,  -8.8342, -10.5487,  ...,  -7.4354,  -8.5653,  -5.2245]]],


        [[[ -6.4085,  -6.6688,  -7.8638,  ...,  -5.3814,  -4.0650,  -6.3453],
          [ -7.5755,  -7.6540,  -9.1397,  ...,  -6.8693,  -6.3328,  -6.3797],
          [ -7.0236,  -7.0181,  -7.1824,  ...,  -5.2739,  -4.4781,  -4.9231],
          ...,
          [ -6.1874,  -7.2829,  -8.0199,  ...,  -5.0066,  -3.6658,  -5.4015],
          [ -6.7833,  -7.1452,  -8.4110,  ...,  -5.0768,  -2.4270,  -4.4857],
          [ -8.2466,  -6.7577,  -9.0418,  ...,  -8.0321,  -8.3991,  -6.0800]]],


        [[[ -6.6740,  -6.8550,  -7.4249,  ...,  -5.3873,  -4.0063,  -6.0456],
          [ -8.2307,  -8.7797,  -9.6736,  ...,  -6.8534,  -6.3120,  -6.1569],
          [ -6.9985,  -7.7832,  -7.3191,  ...,  -6.0313,  -5.4289,  -4.7058],
          ...,
          [ -6.7921,  -7.5872,  -8.1424,  ...,  -5.5530,  -4.4900,  -5.5222],
          [ -6.7865,  -7.3178,  -8.0208,  ...,  -4.8859,  -2.4303,  -4.6603],
          [ -8.2956,  -8.2262,  -9.3742,  ...,  -8.2360,  -8.4217,  -5.7363]]],


        [[[ -6.5553,  -6.8340,  -7.7428,  ...,  -5.4254,  -4.2964,  -6.5379],
          [ -7.9501,  -8.2755,  -8.9417,  ...,  -6.7575,  -6.0531,  -5.6995],
          [ -7.0898,  -7.4158,  -7.6824,  ...,  -5.7893,  -5.6694,  -4.3440],
          ...,
          [ -6.5734,  -7.4929,  -8.0736,  ...,  -5.8067,  -4.7991,  -6.0933],
          [ -6.7027,  -7.5371,  -7.9717,  ...,  -5.4920,  -2.4812,  -4.7149],
          [ -8.2690,  -8.2965,  -8.9206,  ...,  -7.7796,  -7.2990,  -5.5669]]]],
       device='cuda:0'), 'all_bbox_preds': tensor([[[[ 1.2288e+01, -4.5000e+01, -4.3435e-01,  ...,  7.6881e-02,
           -6.8634e-03, -1.3858e-02],
          [ 1.7496e+01,  6.9438e+00, -7.8725e-01,  ..., -5.9164e-01,
           -1.7652e-01,  5.5126e-01],
          [-2.5398e+00,  2.8449e+00, -3.4765e-01,  ..., -7.5645e-01,
            3.2936e-03,  3.4975e-01],
          ...,
          [ 1.8085e+01,  1.9300e+01, -3.9322e-01,  ..., -7.2883e-02,
            1.8560e-03,  1.2295e-05],
          [-1.9120e+01,  4.6640e+01, -4.8116e-01,  ...,  1.6176e-01,
           -4.3152e-03, -1.9949e-05],
          [ 9.8019e+00,  2.6537e+00, -9.9122e-01,  ..., -4.6659e-01,
           -1.5735e-03,  1.1591e-05]]],


        [[[ 1.2431e+01, -4.5028e+01, -3.8177e-01,  ...,  9.4641e-01,
            6.0985e-03, -8.4956e-01],
          [ 1.7753e+01,  6.1897e+00, -5.0031e-01,  ..., -2.3986e-01,
           -4.7061e-01, -2.9798e-01],
          [-2.9273e+00,  2.1536e+00, -4.1348e-01,  ..., -2.2453e-01,
           -6.1603e-03,  7.1786e-02],
          ...,
          [ 1.8100e+01,  1.9243e+01, -4.3204e-01,  ..., -9.8076e-02,
            5.9947e-03, -1.0466e-05],
          [-2.0041e+01,  4.6562e+01, -4.1824e-01,  ...,  5.4498e-01,
           -9.5847e-04, -7.7907e-05],
          [ 9.9673e+00,  2.3657e+00, -9.9156e-01,  ..., -5.2018e-01,
            8.5608e-04, -3.2099e-05]]],


        [[[ 1.2302e+01, -4.5126e+01, -3.3248e-01,  ...,  7.2931e-01,
           -6.8894e-05, -2.9067e-01],
          [ 1.7468e+01,  5.7937e+00, -2.9491e-01,  ..., -2.5885e-01,
           -1.9344e-01, -1.5089e-01],
          [-3.1478e+00,  1.6228e+00, -4.1061e-01,  ..., -8.8129e-01,
           -2.1080e-03,  6.9140e-01],
          ...,
          [ 1.7821e+01,  1.9237e+01, -4.6039e-01,  ..., -4.7880e-01,
            4.6197e-01, -2.8510e-02],
          [-2.0183e+01,  4.6588e+01, -4.7616e-01,  ...,  1.8202e-01,
           -3.4654e-05,  1.3156e-03],
          [ 1.0078e+01,  2.4321e+00, -8.4489e-01,  ..., -7.1851e-01,
            1.7834e-05, -1.1431e-03]]],


        [[[ 1.2135e+01, -4.5098e+01, -3.7295e-01,  ...,  8.1127e-01,
            3.3484e-03, -1.4871e-01],
          [ 1.7347e+01,  5.8987e+00, -5.2688e-01,  ..., -4.3308e-01,
            2.9028e-03, -2.5078e-04],
          [-3.2695e+00,  1.6132e+00, -3.5086e-01,  ..., -9.3409e-01,
            7.4131e-05,  2.3079e-01],
          ...,
          [ 1.7758e+01,  1.9588e+01, -3.9549e-01,  ..., -1.6921e-01,
            2.1941e-01,  1.9887e-01],
          [-1.9697e+01,  4.6604e+01, -4.5951e-01,  ..., -5.4540e-02,
            5.7597e-05,  2.9399e-04],
          [ 9.8859e+00,  2.6862e+00, -6.7902e-01,  ..., -9.5306e-01,
            1.9640e-03, -1.5662e-04]]],


        [[[ 1.1821e+01, -4.5136e+01, -3.7935e-01,  ...,  6.0762e-01,
           -9.7065e-02, -4.9753e-01],
          [ 1.6915e+01,  5.6984e+00, -5.1022e-01,  ..., -5.7583e-01,
            2.9271e-03, -6.6931e-05],
          [-3.5998e+00,  1.7942e+00, -7.0408e-01,  ..., -7.1762e-01,
           -1.1702e-04,  1.4803e-05],
          ...,
          [ 1.7803e+01,  1.9479e+01, -4.2104e-01,  ..., -1.9511e-01,
            5.0120e-01, -1.1002e-01],
          [-1.9671e+01,  4.6638e+01, -4.7161e-01,  ..., -9.5801e-02,
            1.9495e-01, -1.6019e-02],
          [ 9.5398e+00,  2.5874e+00, -7.4525e-01,  ..., -6.4599e-01,
            1.7064e-05,  1.6604e-05]]],


        [[[ 1.1815e+01, -4.5148e+01, -3.5904e-01,  ...,  2.9997e-01,
           -2.1219e-04, -9.2375e-02],
          [ 1.7047e+01,  5.5288e+00, -7.8396e-01,  ..., -2.9071e-01,
           -1.8034e-03, -3.6728e-04],
          [-3.4777e+00,  1.8759e+00, -8.2728e-01,  ..., -8.6339e-01,
           -5.0232e-06,  2.8234e-05],
          ...,
          [ 1.7762e+01,  1.9540e+01, -4.9509e-01,  ...,  2.8659e-01,
            5.3703e-01, -8.6744e-02],
          [-1.9753e+01,  4.6689e+01, -4.7175e-01,  ...,  1.1264e-01,
            7.0443e-02,  4.8112e-03],
          [ 9.4955e+00,  2.3379e+00, -7.6182e-01,  ..., -6.3279e-01,
            1.1132e-05,  9.9912e-06]]]], device='cuda:0'), 'enc_cls_scores': None, 'enc_bbox_preds': None}

 all_cs_scores shape : torch.Size([6, 1, 900, 10])

 all_bbox_preds shape : torch.Size([6, 1, 900, 10])

 ENTERED BEVFORMER HEAD INIT : 

 ENTERED BEVFORMER HEAD INIT_LAYERS : 

 Printing inside the BEVFormer_Head _init_layers______________________________________________ : 

 CLS branch: : [Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=256, bias=True), LayerNorm((256,), eps=1e-05, elementwise_affine=True), ReLU(inplace=True), Linear(in_features=256, out_features=10, bias=True)]

 REG branch: : Sequential(
  (0): Linear(in_features=256, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): ReLU()
  (4): Linear(in_features=256, out_features=10, bias=True)
)

 Printing inside the BEVFormer_Head init___________________________________________________ : 

 __dict__ of BEVFormer Head : {'bev_h': 50, 'bev_w': 50, 'fp16_enabled': False, 'with_box_refine': True, 'as_two_stage': False, 'code_size': 10, 'bbox_coder': <projects.mmdet3d_plugin.core.bbox.coders.nms_free_coder.NMSFreeCoder object at 0x7f8af601c9a0>, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'real_w': 102.4, 'real_h': 102.4, 'num_cls_fcs': 1, 'training': True, '_parameters': OrderedDict([('code_weights', Parameter containing:
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000,
        0.2000]))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict([('loss_cls', FocalLoss()), ('loss_bbox', L1Loss()), ('loss_iou', GIoULoss()), ('activate', ReLU(inplace=True)), ('positional_encoding', LearnedPositionalEncoding(num_feats=128, row_num_embed=50, col_num_embed=50)), ('transformer', PerceptionTransformer(
  (encoder): BEVFormerEncoder(
    (layers): ModuleList(
      (0): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BEVFormerLayer(
        (attentions): ModuleList(
          (0): TemporalSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
            (attention_weights): Linear(in_features=512, out_features=64, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (1): SpatialCrossAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (deformable_attention): MSDeformableAttention3D(
              (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
              (attention_weights): Linear(in_features=256, out_features=64, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (decoder): DetectionTransformerDecoder(
    (layers): ModuleList(
      (0): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): DetrTransformerDecoderLayer(
        (attentions): ModuleList(
          (0): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.1, inplace=False)
          )
          (1): CustomMSDeformableAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
            (attention_weights): Linear(in_features=256, out_features=32, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ffns): ModuleList(
          (0): FFN(
            (activate): ReLU(inplace=True)
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.1, inplace=False)
              )
              (1): Linear(in_features=512, out_features=256, bias=True)
              (2): Dropout(p=0.1, inplace=False)
            )
            (dropout_layer): Identity()
          )
        )
        (norms): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (can_bus_mlp): Sequential(
    (0): Linear(in_features=18, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=256, bias=True)
    (3): ReLU(inplace=True)
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)), ('cls_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=256, bias=True)
    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('reg_branches', ModuleList(
  (0): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (1): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (4): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
  (5): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=10, bias=True)
  )
)), ('bev_embedding', Embedding(2500, 256)), ('query_embedding', Embedding(900, 512))]), '_is_init': False, 'init_cfg': None, 'bg_cls_weight': 0, 'sync_cls_avg_factor': True, 'num_query': 900, 'num_classes': 10, 'in_channels': 256, 'num_reg_fcs': 2, 'train_cfg': None, 'test_cfg': None, 'cls_out_channels': 10, 'act_cfg': {'type': 'ReLU', 'inplace': True}, 'embed_dims': 256}

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 ENTERED BEVFORMER HEAD FORWARD : 

 Printing inside the BEVFormer_Head forward___________________________________________________ : 

 Query embedding: : Embedding(900, 512)

 Object query embeds: : Parameter containing:
tensor([[-0.4259, -1.3660, -0.2644,  ...,  0.0852, -0.1509,  0.4264],
        [ 0.4416,  0.4936,  0.5256,  ..., -0.2080,  1.5487, -0.7089],
        [ 1.1582, -0.1082, -1.1008,  ..., -0.0726, -1.0095, -0.6947],
        ...,
        [ 0.8768, -0.1981,  1.1983,  ..., -0.0531,  1.1525, -0.2061],
        [ 0.2863,  0.9710,  0.6123,  ..., -0.8295,  1.3894,  0.1734],
        [ 0.7548,  1.0330,  1.8136,  ..., -0.3277,  0.6175,  0.8807]],
       device='cuda:0')

 Object query embeds shape: torch.Size([900, 512]) : 

 BEV Qs: : Parameter containing:
tensor([[-0.7539, -0.8606, -1.5688,  ...,  0.5348, -0.3786, -1.7318],
        [ 0.0436,  0.1424, -0.1229,  ..., -0.7287,  1.0243, -0.1771],
        [ 0.7314,  1.1423,  0.2060,  ...,  0.9336,  1.3491,  0.1780],
        ...,
        [-1.4946,  1.5187, -0.4492,  ...,  0.6383,  0.9608,  0.8085],
        [ 0.2285,  0.2309,  0.9725,  ...,  0.2353, -0.1845,  0.0686],
        [-0.1813,  0.0787, -0.1963,  ..., -0.6626,  0.8098,  0.0484]],
       device='cuda:0')

 BEV Qs shape: torch.Size([2500, 256]) : 

 BEV mask: : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')

 BEV mask shape: torch.Size([1, 50, 50]) : 

 BEV pos: : tensor([[[[ 1.1632, -0.5398,  2.4410,  ..., -1.1669, -1.2498,  1.3227],
          [ 1.1632, -0.5398,  2.4410,  ..., -1.1669, -1.2498,  1.3227],
          [ 1.1632, -0.5398,  2.4410,  ..., -1.1669, -1.2498,  1.3227],
          ...,
          [ 1.1632, -0.5398,  2.4410,  ..., -1.1669, -1.2498,  1.3227],
          [ 1.1632, -0.5398,  2.4410,  ..., -1.1669, -1.2498,  1.3227],
          [ 1.1632, -0.5398,  2.4410,  ..., -1.1669, -1.2498,  1.3227]],

         [[ 0.2218,  0.2276, -0.7997,  ..., -0.3673, -0.4338,  0.4864],
          [ 0.2218,  0.2276, -0.7997,  ..., -0.3673, -0.4338,  0.4864],
          [ 0.2218,  0.2276, -0.7997,  ..., -0.3673, -0.4338,  0.4864],
          ...,
          [ 0.2218,  0.2276, -0.7997,  ..., -0.3673, -0.4338,  0.4864],
          [ 0.2218,  0.2276, -0.7997,  ..., -0.3673, -0.4338,  0.4864],
          [ 0.2218,  0.2276, -0.7997,  ..., -0.3673, -0.4338,  0.4864]],

         [[-0.9746, -0.9412,  1.8879,  ..., -0.6675,  0.2121, -1.7216],
          [-0.9746, -0.9412,  1.8879,  ..., -0.6675,  0.2121, -1.7216],
          [-0.9746, -0.9412,  1.8879,  ..., -0.6675,  0.2121, -1.7216],
          ...,
          [-0.9746, -0.9412,  1.8879,  ..., -0.6675,  0.2121, -1.7216],
          [-0.9746, -0.9412,  1.8879,  ..., -0.6675,  0.2121, -1.7216],
          [-0.9746, -0.9412,  1.8879,  ..., -0.6675,  0.2121, -1.7216]],

         ...,

         [[ 1.0568,  1.0568,  1.0568,  ...,  1.0568,  1.0568,  1.0568],
          [ 0.2385,  0.2385,  0.2385,  ...,  0.2385,  0.2385,  0.2385],
          [ 0.6219,  0.6219,  0.6219,  ...,  0.6219,  0.6219,  0.6219],
          ...,
          [ 0.2538,  0.2538,  0.2538,  ...,  0.2538,  0.2538,  0.2538],
          [-0.2278, -0.2278, -0.2278,  ..., -0.2278, -0.2278, -0.2278],
          [-0.7648, -0.7648, -0.7648,  ..., -0.7648, -0.7648, -0.7648]],

         [[ 0.9530,  0.9530,  0.9530,  ...,  0.9530,  0.9530,  0.9530],
          [ 1.8456,  1.8456,  1.8456,  ...,  1.8456,  1.8456,  1.8456],
          [-0.2594, -0.2594, -0.2594,  ..., -0.2594, -0.2594, -0.2594],
          ...,
          [-0.2839, -0.2839, -0.2839,  ..., -0.2839, -0.2839, -0.2839],
          [ 0.0471,  0.0471,  0.0471,  ...,  0.0471,  0.0471,  0.0471],
          [ 0.5158,  0.5158,  0.5158,  ...,  0.5158,  0.5158,  0.5158]],

         [[ 1.3177,  1.3177,  1.3177,  ...,  1.3177,  1.3177,  1.3177],
          [ 0.3279,  0.3279,  0.3279,  ...,  0.3279,  0.3279,  0.3279],
          [-0.2635, -0.2635, -0.2635,  ..., -0.2635, -0.2635, -0.2635],
          ...,
          [ 0.0324,  0.0324,  0.0324,  ...,  0.0324,  0.0324,  0.0324],
          [-0.1789, -0.1789, -0.1789,  ..., -0.1789, -0.1789, -0.1789],
          [-0.5444, -0.5444, -0.5444,  ..., -0.5444, -0.5444, -0.5444]]]],
       device='cuda:0')

 BEV pos shape: torch.Size([1, 256, 50, 50]) : 

 OUTPUTS : (tensor([[[-0.5130,  0.1260, -0.2230,  ...,  0.1015,  0.2318,  0.4409]],

        [[-0.6056,  0.5826, -0.5336,  ...,  0.0307,  0.5142,  0.1121]],

        [[-1.0551,  0.2234,  0.2354,  ...,  0.8887,  0.8728,  0.2350]],

        ...,

        [[-1.0276,  0.3219, -0.5642,  ...,  0.4516,  1.0663, -0.1035]],

        [[-0.8030,  0.0894, -0.6452,  ...,  0.5155,  0.8652,  0.0386]],

        [[-1.0053,  0.2619, -0.8573,  ...,  0.1975,  0.5337, -0.0500]]],
       device='cuda:0'), tensor([[[[-0.2917,  0.1990,  0.0633,  ...,  1.2492,  0.1158,  1.1541]],

         [[ 0.4791, -0.4563, -0.2587,  ...,  0.7914,  0.1755,  0.5543]],

         [[-0.0971,  0.4789,  0.0160,  ..., -1.0561, -1.0299,  0.7472]],

         ...,

         [[-0.0569, -0.4668, -0.2762,  ...,  1.1367, -0.1801,  0.5201]],

         [[ 0.1028, -0.3258, -0.8476,  ...,  0.7798,  0.7369,  0.3354]],

         [[ 0.1874,  0.6683,  0.3142,  ...,  0.5465, -0.8161,  1.1494]]],


        [[[ 0.5887,  0.0882, -0.7097,  ...,  0.7566,  0.2704,  0.0077]],

         [[ 1.1772, -0.5944, -0.2292,  ...,  0.5614, -0.0087,  0.0031]],

         [[ 0.4487,  0.1058, -0.0091,  ...,  0.6392, -0.9953,  1.1196]],

         ...,

         [[ 1.1223, -0.3525, -0.5341,  ...,  1.0180, -0.5020,  0.0611]],

         [[ 0.5265,  0.2309, -1.0597,  ...,  1.2075,  0.0805, -0.5696]],

         [[ 0.9036,  0.3949,  0.2124,  ...,  1.0795,  0.0408,  0.9112]]],


        [[[ 0.2988,  0.6813, -0.1227,  ...,  0.4535, -0.0911, -0.6194]],

         [[ 0.6126, -0.1126, -0.0317,  ...,  0.0147, -0.4221, -0.4253]],

         [[-0.0252, -0.2583, -0.5274,  ..., -0.2163, -1.2318,  0.2558]],

         ...,

         [[ 0.7258,  0.2267, -0.3377,  ...,  0.8545, -0.5998, -0.3781]],

         [[ 0.6639,  0.3026, -0.9302,  ...,  1.0656, -0.4561, -1.0808]],

         [[-0.2883, -0.1607, -0.1410,  ...,  0.0132, -0.5653, -0.2623]]],


        [[[ 0.2972,  0.7397,  0.9966,  ...,  0.9889, -0.6654,  0.2111]],

         [[ 0.1025,  0.5409,  0.5727,  ..., -0.0818, -0.6908, -0.1992]],

         [[ 0.4877,  0.4536,  0.2187,  ..., -0.0297, -2.1110, -0.4792]],

         ...,

         [[ 0.7929,  0.4106,  0.8156,  ...,  1.1051, -0.7246,  0.3451]],

         [[ 1.4315, -0.4230,  0.2996,  ...,  0.5357, -0.6078, -1.0373]],

         [[-0.3523,  0.6430,  0.5174,  ..., -0.1058, -0.8125, -0.0165]]],


        [[[ 0.7100,  0.2798,  0.7392,  ...,  0.8498, -1.4065, -0.3458]],

         [[ 0.7854,  0.8525,  0.7362,  ..., -0.2188, -2.1652, -0.3107]],

         [[ 1.5614,  1.3657,  0.5639,  ..., -0.4019, -3.0796, -0.2083]],

         ...,

         [[ 1.4263,  0.7187,  0.6597,  ...,  0.7464, -1.9152, -0.0447]],

         [[ 1.3929, -0.5425, -0.6785,  ...,  0.5156, -1.2243, -0.9519]],

         [[ 0.9584,  0.7949,  0.7318,  ...,  0.1668, -2.5886, -0.4432]]],


        [[[ 1.7363,  0.7242,  1.0077,  ..., -0.2672, -0.9678, -0.4497]],

         [[ 1.6764,  1.2458,  1.2072,  ..., -1.2515, -1.3753, -0.1641]],

         [[ 1.9122,  2.1977,  0.6852,  ..., -0.8786, -2.0437, -0.6005]],

         ...,

         [[ 1.7468,  1.2090,  1.1926,  ..., -0.3496, -1.2619,  0.1276]],

         [[ 1.9571, -0.1542,  0.7654,  ..., -0.2447, -0.8975, -0.5391]],

         [[ 1.5765,  1.2691,  0.3951,  ..., -0.4112, -1.4840, -0.8616]]]],
       device='cuda:0'), tensor([[[0.6264, 0.0600, 0.3168],
         [0.6620, 0.5795, 0.5226],
         [0.4859, 0.5455, 0.5059],
         ...,
         [0.6724, 0.7027, 0.5622],
         [0.3057, 0.9592, 0.6979],
         [0.5928, 0.5281, 0.5059]]], device='cuda:0'), tensor([[[[0.6051, 0.0584, 0.2705],
          [0.6655, 0.5849, 0.6156],
          [0.4910, 0.5370, 0.5329],
          ...,
          [0.6691, 0.6985, 0.7878],
          [0.3173, 0.9599, 0.8011],
          [0.5603, 0.5303, 0.5611]]],


        [[[0.6013, 0.0576, 0.2752],
          [0.6664, 0.5806, 0.6230],
          [0.4953, 0.5326, 0.5359],
          ...,
          [0.6694, 0.6914, 0.7974],
          [0.3161, 0.9603, 0.8047],
          [0.5573, 0.5262, 0.5708]]],


        [[[0.5984, 0.0596, 0.2730],
          [0.6688, 0.5854, 0.6264],
          [0.4990, 0.5337, 0.5321],
          ...,
          [0.6703, 0.6980, 0.7965],
          [0.3118, 0.9609, 0.8037],
          [0.5579, 0.5334, 0.5646]]],


        [[[0.5977, 0.0612, 0.2634],
          [0.6684, 0.5925, 0.6217],
          [0.5005, 0.5368, 0.5418],
          ...,
          [0.6678, 0.7048, 0.7850],
          [0.3082, 0.9640, 0.7986],
          [0.5573, 0.5398, 0.5649]]],


        [[[0.5945, 0.0608, 0.2535],
          [0.6690, 0.5928, 0.6153],
          [0.5024, 0.5377, 0.5390],
          ...,
          [0.6640, 0.7047, 0.7785],
          [0.3066, 0.9644, 0.7999],
          [0.5488, 0.5440, 0.5556]]],


        [[[0.5930, 0.0606, 0.2531],
          [0.6704, 0.5933, 0.6132],
          [0.5047, 0.5403, 0.5423],
          ...,
          [0.6636, 0.7050, 0.7786],
          [0.3076, 0.9644, 0.8003],
          [0.5489, 0.5461, 0.5509]]]], device='cuda:0'))

 BEV embed: : tensor([[[-0.5130,  0.1260, -0.2230,  ...,  0.1015,  0.2318,  0.4409]],

        [[-0.6056,  0.5826, -0.5336,  ...,  0.0307,  0.5142,  0.1121]],

        [[-1.0551,  0.2234,  0.2354,  ...,  0.8887,  0.8728,  0.2350]],

        ...,

        [[-1.0276,  0.3219, -0.5642,  ...,  0.4516,  1.0663, -0.1035]],

        [[-0.8030,  0.0894, -0.6452,  ...,  0.5155,  0.8652,  0.0386]],

        [[-1.0053,  0.2619, -0.8573,  ...,  0.1975,  0.5337, -0.0500]]],
       device='cuda:0')

 BEV embed shape : torch.Size([2500, 1, 256])

 OUTS: : {'bev_embed': tensor([[[-0.5130,  0.1260, -0.2230,  ...,  0.1015,  0.2318,  0.4409]],

        [[-0.6056,  0.5826, -0.5336,  ...,  0.0307,  0.5142,  0.1121]],

        [[-1.0551,  0.2234,  0.2354,  ...,  0.8887,  0.8728,  0.2350]],

        ...,

        [[-1.0276,  0.3219, -0.5642,  ...,  0.4516,  1.0663, -0.1035]],

        [[-0.8030,  0.0894, -0.6452,  ...,  0.5155,  0.8652,  0.0386]],

        [[-1.0053,  0.2619, -0.8573,  ...,  0.1975,  0.5337, -0.0500]]],
       device='cuda:0'), 'all_cls_scores': tensor([[[[-8.5090, -8.2281, -8.0147,  ..., -6.9039, -6.0903, -6.0908],
          [-8.9074, -9.1708, -8.4026,  ..., -6.3680, -6.0302, -5.7576],
          [-6.9456, -8.3013, -8.8090,  ..., -6.1420, -5.8701, -6.6366],
          ...,
          [-8.4234, -8.9616, -9.3628,  ..., -7.1790, -7.1942, -7.1460],
          [-7.9003, -8.7235, -9.3233,  ..., -5.7050, -5.3085, -7.1080],
          [-9.8961, -9.7028, -9.0986,  ..., -6.8844, -6.0947, -6.8963]]],


        [[[-8.0104, -7.2600, -8.5328,  ..., -6.4529, -5.7416, -5.6461],
          [-7.5154, -7.6782, -8.5517,  ..., -6.3538, -6.2504, -5.2687],
          [-6.7910, -7.3127, -9.3781,  ..., -6.8825, -6.3872, -7.0778],
          ...,
          [-7.3310, -7.4063, -9.1725,  ..., -6.7042, -6.3870, -6.2571],
          [-7.7768, -7.3581, -9.0631,  ..., -5.6072, -4.9257, -5.9142],
          [-8.0654, -8.3058, -9.3046,  ..., -6.7795, -7.4105, -6.3821]]],


        [[[-8.5819, -7.9609, -8.5128,  ..., -6.5654, -5.6286, -5.8343],
          [-8.1045, -8.2543, -8.5992,  ..., -6.0191, -5.8870, -5.4267],
          [-8.2832, -8.3399, -9.2439,  ..., -6.8523, -6.7030, -6.7479],
          ...,
          [-7.5629, -7.7632, -8.9403,  ..., -6.6279, -6.3719, -6.9601],
          [-7.8205, -7.7954, -8.4475,  ..., -5.3049, -5.0929, -6.1600],
          [-8.8723, -8.6790, -9.3320,  ..., -6.5376, -6.9068, -6.1537]]],


        [[[-7.5502, -7.2838, -7.8016,  ..., -6.9619, -6.4494, -5.6919],
          [-7.2421, -7.6910, -7.6547,  ..., -6.4120, -6.2469, -5.4602],
          [-7.3786, -7.8289, -7.8002,  ..., -6.9974, -6.6913, -6.7171],
          ...,
          [-7.4399, -7.7572, -8.7924,  ..., -7.0133, -6.5201, -6.6183],
          [-7.0501, -7.5437, -8.7970,  ..., -5.4221, -4.7954, -5.6769],
          [-8.2317, -7.9176, -7.7653,  ..., -7.0695, -7.4889, -5.9746]]],


        [[[-8.2123, -7.9712, -8.6006,  ..., -6.9943, -6.0420, -5.6754],
          [-7.8612, -8.2447, -9.0188,  ..., -6.1424, -5.7372, -5.5559],
          [-8.0507, -8.4247, -9.7017,  ..., -6.4800, -6.1067, -6.1382],
          ...,
          [-7.7603, -8.1562, -9.6211,  ..., -6.8263, -6.3853, -6.6602],
          [-7.4975, -8.0393, -9.1084,  ..., -5.6945, -4.9248, -5.9709],
          [-8.7979, -8.7554, -9.3646,  ..., -7.0174, -7.0718, -5.9273]]],


        [[[-8.2581, -8.0787, -8.0858,  ..., -7.1502, -6.0239, -5.6361],
          [-7.6725, -8.0268, -8.0602,  ..., -6.2445, -5.6348, -5.3537],
          [-7.8866, -7.8727, -8.3487,  ..., -7.0325, -5.8936, -6.1679],
          ...,
          [-7.4502, -7.9198, -8.6834,  ..., -7.2220, -6.7984, -6.8615],
          [-7.4795, -7.8536, -8.6777,  ..., -5.7914, -4.7400, -6.0035],
          [-8.1420, -8.2262, -8.6147,  ..., -7.0531, -6.7985, -5.7282]]]],
       device='cuda:0'), 'all_bbox_preds': tensor([[[[ 1.0767e+01, -4.5216e+01, -6.3431e-01,  ..., -9.0007e-02,
           -2.3552e-03,  9.7684e-06],
          [ 1.6948e+01,  8.6959e+00, -8.7083e-01,  ..., -5.2361e-01,
            1.2655e-03, -5.8964e-05],
          [-9.1978e-01,  3.7854e+00, -3.1893e-01,  ..., -8.7017e-01,
            5.7142e-03,  1.1813e+00],
          ...,
          [ 1.7311e+01,  2.0331e+01, -6.4416e-01,  ..., -4.5620e-01,
           -2.1573e-03, -1.4028e-05],
          [-1.8710e+01,  4.7098e+01, -3.4496e-01,  ...,  4.0415e-01,
            1.4553e-02, -6.7710e-03],
          [ 6.1785e+00,  3.1007e+00, -3.2837e-01,  ..., -6.2260e-01,
            5.3209e-03,  3.4523e-01]]],


        [[[ 1.0373e+01, -4.5305e+01, -2.8408e-01,  ...,  1.6399e-01,
           -1.1006e-04,  2.1067e-05],
          [ 1.7043e+01,  8.2531e+00, -6.3835e-01,  ..., -8.1256e-02,
            3.1759e-03,  5.1019e-06],
          [-4.8417e-01,  3.3431e+00, -1.7738e-01,  ..., -7.8525e-01,
            5.3750e-02,  3.6523e-01],
          ...,
          [ 1.7347e+01,  1.9604e+01, -1.1050e-02,  ..., -1.2878e-03,
            1.8246e-03, -1.3546e-01],
          [-1.8835e+01,  4.7131e+01, -3.6642e-01,  ...,  7.6274e-01,
            7.4445e-04, -8.1487e-06],
          [ 5.8657e+00,  2.6828e+00, -2.1366e-02,  ...,  9.7794e-02,
           -3.1438e-03, -2.8513e-02]]],


        [[[ 1.0078e+01, -4.5093e+01, -3.6794e-01,  ..., -2.4498e-02,
           -7.8176e-05,  2.8379e-03],
          [ 1.7289e+01,  8.7484e+00, -5.5227e-01,  ..., -2.8956e-01,
           -1.3647e-04, -1.6688e-02],
          [-1.0675e-01,  3.4459e+00, -2.4697e-01,  ..., -6.6280e-01,
            3.5475e-03,  4.4345e-02],
          ...,
          [ 1.7443e+01,  2.0277e+01,  2.4962e-02,  ..., -1.7157e-01,
            2.0092e-02, -6.6128e-02],
          [-1.9272e+01,  4.7199e+01, -4.2615e-01,  ...,  5.2544e-01,
            1.2245e-04, -3.9379e-02],
          [ 5.9244e+00,  3.4217e+00, -4.0784e-01,  ...,  5.0845e-01,
            3.4571e-04, -3.1567e-01]]],


        [[[ 1.0009e+01, -4.4937e+01,  2.6137e-01,  ...,  7.1544e-02,
           -5.4894e-06,  1.0987e-04],
          [ 1.7244e+01,  9.4681e+00, -1.9947e-01,  ..., -5.9970e-02,
            9.9167e-06,  1.5216e-04],
          [ 5.4699e-02,  3.7667e+00, -3.0388e-01,  ..., -6.1826e-01,
            1.4725e-06,  2.6096e-05],
          ...,
          [ 1.7184e+01,  2.0969e+01,  1.7065e-01,  ...,  7.3037e-03,
            7.8291e-07,  7.7094e-05],
          [-1.9636e+01,  4.7518e+01, -1.9867e-01,  ...,  6.2565e-01,
            2.5645e-02, -2.7102e-02],
          [ 5.8713e+00,  4.0753e+00, -4.5954e-01,  ...,  2.9131e-01,
            2.3381e-05,  6.0677e-06]]],


        [[[ 9.6717e+00, -4.4970e+01, -2.4586e-01,  ...,  3.4771e-02,
           -6.6787e-04, -2.0833e-04],
          [ 1.7303e+01,  9.4976e+00, -6.6341e-01,  ..., -1.0621e-01,
           -4.1127e-04,  7.5002e-06],
          [ 2.5085e-01,  3.8622e+00, -3.3476e-01,  ..., -6.6990e-01,
            1.1419e-05,  2.4754e-05],
          ...,
          [ 1.6791e+01,  2.0963e+01, -2.7823e-01,  ..., -1.4753e-01,
            4.3407e-04,  1.9399e-05],
          [-1.9805e+01,  4.7550e+01, -3.2851e-01,  ...,  6.4004e-01,
            1.5729e-02, -1.3100e-03],
          [ 4.9923e+00,  4.5092e+00, -7.5650e-01,  ..., -5.2290e-02,
           -9.5465e-04, -3.9492e-05]]],


        [[[ 9.5221e+00, -4.4990e+01, -4.6904e-01,  ...,  8.1954e-02,
            2.9511e-05, -5.9801e-06],
          [ 1.7451e+01,  9.5587e+00, -7.8116e-01,  ...,  5.4058e-02,
           -3.6715e-03, -7.3056e-04],
          [ 4.8518e-01,  4.1283e+00, -2.1677e-01,  ..., -5.5910e-01,
           -1.7324e-03, -3.6250e-04],
          ...,
          [ 1.6752e+01,  2.0988e+01, -3.5671e-01,  ...,  1.9258e-01,
           -2.6099e-05, -3.9745e-05],
          [-1.9701e+01,  4.7550e+01, -2.7282e-01,  ...,  5.2463e-01,
           -7.2097e-04,  7.9264e-03],
          [ 5.0047e+00,  4.7239e+00, -8.8510e-01,  ...,  3.4766e-01,
           -3.2546e-03, -6.6125e-04]]]], device='cuda:0'), 'enc_cls_scores': None, 'enc_bbox_preds': None}

 all_cs_scores shape : torch.Size([6, 1, 900, 10])

 all_bbox_preds shape : torch.Size([6, 1, 900, 10])
